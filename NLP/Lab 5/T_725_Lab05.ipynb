{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zc7Et2_VsbNWotWcA-0XG3PkCHxdFcQL","timestamp":1695986668567}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q3IOeFoye2P8"},"source":["# T-725 Natural Language Processing: Lab 5\n","In today's lab, we will be working with neural networks, using GRUs and Transformers for text generation.\n","\n","To begin with, do the following:\n","* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n","* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n","* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."]},{"cell_type":"code","metadata":{"id":"R7ElxTOtl6UQ","executionInfo":{"status":"ok","timestamp":1695986716330,"user_tz":0,"elapsed":259,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"source":["import os\n","import warnings\n","\n","# Suppress some warnings from TensorFlow about deprecated functions\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ayz3HiU7JvCF"},"source":["## Generating text with neural networks\n","Let's create a neural language model and use it to generate some text. This time, we will use character embeddings rather than word embeddings. They are created in exactly the same way, and are often used together in neural network-based models. One benefit of using character embeddings is that we can generate words that our model has never seen before.\n","\n","The model takes as input a sequence of characters and predicts which character is most likely to follow. We will generate text by repeatedly predicting and appending the next character to a string. First, however, we need some text to train it on.\n"]},{"cell_type":"code","metadata":{"id":"PN7I_djD91Py","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695986734391,"user_tz":0,"elapsed":4989,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"ab8b99ec-6bd6-49f3-b97a-c3d6f192ea24"},"source":["# Based on the following tutorial:\n","# https://www.tensorflow.org/tutorials/text/text_generation\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","\n","# Let's download some text by Shakespeare to train our model\n","url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n","\n","with open(path_to_file, encoding='utf-8') as f:\n","  shakespeare = f.read()\n","\n","print(\"First 250 characters:\")\n","print(shakespeare[:250])\n","\n","print (\"Length of text: {:,} characters\".format(len(shakespeare)))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n","First 250 characters:\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","Length of text: 1,115,394 characters\n"]}]},{"cell_type":"markdown","metadata":{"id":"45JqVDxqtf1o"},"source":["Now we can create training examples for our model. Each example will be a pair of strings: one input string containing 100 characters, and a target string that is one character ahead. For example, the first pair we create is:\n","\n","**Input string**:  `'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'`\n","\n","**Target string**: `'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '`\n","\n","However, before we can start training, we need to convert our text into a list of integers, where each integer represents a different character. For example, \"First Citizen\" becomes:\n","\n","```\n","Character:   F   i   r   s   t      C   i   t   i   z   e   n\n","Integer:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n","```"]},{"cell_type":"code","metadata":{"id":"dWPZjI0xHJ44","executionInfo":{"status":"ok","timestamp":1695986785415,"user_tz":0,"elapsed":318,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"source":["BATCH_SIZE = 64  # Batch size\n","BUFFER_SIZE = 10000  # Buffer size to shuffle the dataset\n","\n","def split_input_target(chunk):\n","  # Create (input_string, output_string) pairs\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","def prepare_text(text):\n","  # The unique characters in the file\n","  vocab = sorted(set(text))\n","  print ('{} unique characters'.format(len(vocab)))\n","\n","  # Creating a mapping from unique characters to indices\n","  char_map = {\n","      'char_to_index': {char: index for index, char in enumerate(vocab)},\n","      'index_to_char': np.array(vocab)\n","  }\n","\n","  text_as_int = np.array([char_map['char_to_index'][c] for c in text])\n","\n","  # The maximum length sentence we want for a single input in characters\n","  seq_length = 100\n","  examples_per_epoch = len(text) // (seq_length+1)\n","\n","  # Create training examples / targets\n","  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","  dataset = sequences.map(split_input_target)\n","\n","  # (TF data is designed to work with possibly infinite sequences,\n","  # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","  # it maintains a buffer in which it shuffles elements).\n","  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","  return dataset, vocab, examples_per_epoch, char_map"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPGoaE2TDnX9"},"source":["Now we can create and train the neural network."]},{"cell_type":"code","metadata":{"id":"y5bOTe1hDqtY","executionInfo":{"status":"ok","timestamp":1695988253570,"user_tz":0,"elapsed":5,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"source":["import os\n","\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Embedding(vocab_size,\n","                                embedding_dim,\n","                                batch_input_shape=[batch_size, None]),\n","      tf.keras.layers.GRU(rnn_units,\n","                          return_sequences=True,\n","                          recurrent_initializer='glorot_uniform',\n","                          stateful=True),\n","      tf.keras.layers.Dense(vocab_size)\n","  ])\n","\n","  return model\n","\n","\n","def create_model(text, epochs=3, embedding_dim = 256, rnn_units = 1024):\n","  dataset, vocab, examples_per_epoch, char_map = prepare_text(text)\n","\n","  vocab_size = len(vocab)  # Length of the vocabulary in chars\n","\n","  model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss=loss)\n","\n","  # Create checkpoints once the model has been trained\n","  checkpoint_dir = './training_checkpoints'\n","  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","      filepath=checkpoint_prefix,\n","      save_weights_only=True)\n","\n","  # Train the model\n","  history = model.fit(\n","      dataset,\n","      epochs=epochs,\n","      callbacks=[checkpoint_callback])\n","\n","  tf.train.latest_checkpoint(checkpoint_dir)\n","  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","  model.build(tf.TensorShape([1, None]))\n","\n","  return model, char_map"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"tK4fcZI55rzd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695986885692,"user_tz":0,"elapsed":71392,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"9b4f2358-d461-4b43-e62b-5c106f3122ca"},"source":["shake_model, shake_chars = create_model(shakespeare)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n","Epoch 1/3\n","172/172 [==============================] - 22s 61ms/step - loss: 2.6645\n","Epoch 2/3\n","172/172 [==============================] - 11s 52ms/step - loss: 1.9584\n","Epoch 3/3\n","172/172 [==============================] - 11s 52ms/step - loss: 1.6903\n"]}]},{"cell_type":"markdown","metadata":{"id":"m_R1efC2eKH1"},"source":["Now that we've trained our model, we can finally use it to generate some text. The following function takes a model and a string as input, and continually predicts and appends the next character to the string until it becomes 1,000 characters long."]},{"cell_type":"code","metadata":{"id":"KhVcs5ny-urX","executionInfo":{"status":"ok","timestamp":1695987385298,"user_tz":0,"elapsed":317,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"source":["def generate_text(model, char_map, start_string, temperature=1.0, num_generate = 1000):\n","    # Evaluation step (generating text using the learned model)\n","    # Low temperatures results in more predictable text.\n","    # Higher temperatures results in more surprising text.\n","    if not start_string:\n","        print(\"start_string can't be empty\")\n","        return \"\"\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char_map['char_to_index'][s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # Empty string to store our results\n","    text_generated = [\"\"] * num_generate\n","\n","    # Here batch size == 1\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the character returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # We pass the predicted word as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated[i] = char_map['index_to_char'][predicted_id]\n","\n","    return (start_string + ''.join(text_generated))"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kS0UlqVbhOwa"},"source":["Let's generate some text!"]},{"cell_type":"code","metadata":{"id":"1YOnJYAn-upC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695987056787,"user_tz":0,"elapsed":8880,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"6fe529ae-a0db-4689-ad81-fa90e991fc34"},"source":["print(generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=1.0))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO: that not the moly tome:\n","The proters of piece and teasure farthough, remeant strak.\n","Till: the medy of for whoshings the Heard I have you; ear hearantis\n","fothind holes of yours in liage.\n","\n","CLINFORDEY:\n","Shile I have deach, you queen smeer.\n","\n","KINGURIIA:\n","Ballay, lo, hash voward, xit fornes you what is to musca,\n","He is the boting do be feathis.\n","\n","HERMIONE:\n","O, gentle,--his lime.\n","\n","GLOUCESTIO:\n","Now, that you mean a\n","promence merncio endy?\n","\n","WISTHES MIIRAND:\n","Vencless of call, affer prame\n","yiur entigred and fetty. mark is bug her\n","We twoo, been, when sheal, when I strak.\n","\n","NICHAMDINE:\n","First I and kiss upon tho biving was.\n","\n","JUCHESS OF YORK:\n","End with yet.\n","\n","DUKE OF AUEE:\n","Madam, the stread wey to cut one; rid that woo, When I deer epert;\n","Is cried; as him, goin reme but before.\n","\n","SICINIUS:\n","Pet my heart thisp:\n","she stiel not and roth pursapp.\n","\n","JULIET:\n","O herly, My repart.\n","Hip I connent she conterness.\n","Shippinio's me subrece agbear for your will\n","Like makeaid I Then,--haves? Whancestixing heard to a inful;\n","And worthy a\n"]}]},{"cell_type":"markdown","metadata":{"id":"-n5WwsuZe0B6"},"source":["# Assignment\n","Answer the following questions and hand in your solution in Canvas before 8:30 on Monday morning, October 2nd. Remember to save your file before uploading it."]},{"cell_type":"markdown","metadata":{"id":"ZoKGONy4fSl3"},"source":["## Question 1\n","The `temperature` parameter of `generate_text()`, defined earlier in the notebook, controls how predictable the generated text will be. The lower the temperature, the more the function will tend to append the most likely character (according to the model's prediction). A higher temperature introduces some randomness, leading to more unpredictable text.\n","\n","The text we generated above used a temperature of 1.0. Try generating more text using the Shakespeare model, once using a temperature of 0.2 and again using a temperature of 0.8."]},{"cell_type":"code","metadata":{"id":"XsSNVxKEeGZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695987206566,"user_tz":0,"elapsed":18132,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"0fbae834-6a9f-4189-ce93-56efe94018ab"},"source":["# Your solution here\n","text_1 = generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=.2)\n","text_2 = generate_text(shake_model, shake_chars, \"ROMEO: \", temperature=.8)\n","\n","print(\"################### TEMP: 0.2\")\n","print(text_1)\n","\n","print(\"################### TEMP: 0.8\")\n","print(text_2)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["################### TEMP: 0.2\n","ROMEO: the could not the duke of the seath.\n","\n","LEONTES:\n","I do not the did the suppection of the seement.\n","\n","MENENIUS:\n","What is the world the death of the stard of the death.\n","\n","COMINIUS:\n","What is the strange of the world the death.\n","\n","KING RICHARD III:\n","Why, the world when the death of the death, the father with the marries\n","And the death of the comes to the dread with the will in the courter of the world and the courter from the streather with the command.\n","\n","KING RICHARD III:\n","What is the stranger the did of the world and death,\n","And when I shall not the death and leave the trumper of the seath.\n","\n","KING RICHARD III:\n","Who, I have stay the world not the dear of the world be not the commander that well the death.\n","\n","MERCUTIO:\n","A may a thing in the courter that shall be the world,\n","And what the strange of the did of the marries and the more that I shall be not the company.\n","\n","KING RICHARD III:\n","What is the companter the death of the companter to the streather\n","As the death of the death, the death of the world the death,\n","A\n","################### TEMP: 0.8\n","ROMEO: say, why shall be your ears with her heart\n","Of for my hear him death,\n","And they are it net death and loves at that thy fleer?\n","\n","BETHUCHIO:\n","As you upstace; you will thy make and erece to should be come.\n","\n","MERENIUS:\n","What it ferdily: wise you be, in this lear of yet fail about.\n","\n","KATHARINA:\n","I prean the retold him is may the would\n","And become me name would should from not he\n","doy dence that seffed couse didon hath upon your mander fallen,\n","Wherear, but a where in stain the wind!\n","\n","GLOUCESTER:\n","SICINIUS:\n","I am the dake thou, hear in thumpers;\n","I warrant fance of yet wouch the city of with men of the own pair us a lipter accarst\n","She dive your murderous brather.\n","\n","DUKE OF AUMERLE:\n","My lord, I know thy scorting love ty make!\n","\n","Bobind:\n","Ther fear it from this from heppess there.\n","\n","HERBYOUD:\n","And this for a word, you here is hear'd\n","Is charl shall cannot recoting could have the prepity all dreit fear all the fearthen.\n","\n","BUCKINGHAM:\n","Well, he bid.\n","\n","KATHARINA:\n","No, in joot us prane chorgeson; which see the youl mad!\n","\n","M\n"]}]},{"cell_type":"markdown","metadata":{"id":"hauhMD0gfV2o"},"source":["## Question 2\n","NLTK's `names` corpus contains a list of approximately 8,000 English names. Train a new model on `names_raw` for at least 20 epochs using the `create_model(text, epochs=n)` function defined earlier. Use the trained model to generate a list of names (with the `generate_text` function defined earlier), starting with your own first name. Your name should not contain any non-English characters, and should end with an `\\n`.\n","\n","Print out the names that do not appear in the training data. Do you get any actual names (or at least names that sound plausible)?"]},{"cell_type":"code","metadata":{"id":"NEOIqRE9fWTF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695987288620,"user_tz":0,"elapsed":8,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"a6015aba-abb9-4447-ecc2-d85cbfff6bdb"},"source":["# Don't modify this code cell\n","import nltk\n","from nltk.corpus import names\n","nltk.download('names', quiet=True)\n","\n","# Print out a few examples\n","names_raw = names.raw()\n","names_unique = set(names_raw.split())\n","names_raw = \"\\n\".join(names_unique)\n","print(names_raw.splitlines()[:5])"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['Englebert', 'Ahmad', 'Ibrahim', 'Astrid', 'Aylmer']\n"]}]},{"cell_type":"code","metadata":{"id":"d7Y8qE1tYR1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695987684997,"user_tz":0,"elapsed":21063,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"aa44f097-3991-4223-c828-bee22115efdc"},"source":["# Your solution here\n","names_model, names_chars = create_model(names_raw, epochs=20)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["55 unique characters\n","Epoch 1/20\n","8/8 [==============================] - 5s 102ms/step - loss: 4.1898\n","Epoch 2/20\n","8/8 [==============================] - 1s 88ms/step - loss: 3.8069\n","Epoch 3/20\n","8/8 [==============================] - 1s 70ms/step - loss: 3.5636\n","Epoch 4/20\n","8/8 [==============================] - 1s 71ms/step - loss: 3.0584\n","Epoch 5/20\n","8/8 [==============================] - 1s 71ms/step - loss: 2.8259\n","Epoch 6/20\n","8/8 [==============================] - 1s 75ms/step - loss: 2.5997\n","Epoch 7/20\n","8/8 [==============================] - 1s 68ms/step - loss: 2.4582\n","Epoch 8/20\n","8/8 [==============================] - 1s 75ms/step - loss: 2.3815\n","Epoch 9/20\n","8/8 [==============================] - 1s 70ms/step - loss: 2.3374\n","Epoch 10/20\n","8/8 [==============================] - 1s 72ms/step - loss: 2.3035\n","Epoch 11/20\n","8/8 [==============================] - 1s 74ms/step - loss: 2.2772\n","Epoch 12/20\n","8/8 [==============================] - 1s 70ms/step - loss: 2.2528\n","Epoch 13/20\n","8/8 [==============================] - 1s 78ms/step - loss: 2.2329\n","Epoch 14/20\n","8/8 [==============================] - 1s 81ms/step - loss: 2.2153\n","Epoch 15/20\n","8/8 [==============================] - 1s 71ms/step - loss: 2.2022\n","Epoch 16/20\n","8/8 [==============================] - 1s 72ms/step - loss: 2.1878\n","Epoch 17/20\n","8/8 [==============================] - 1s 75ms/step - loss: 2.1792\n","Epoch 18/20\n","8/8 [==============================] - 1s 72ms/step - loss: 2.1660\n","Epoch 19/20\n","8/8 [==============================] - 1s 93ms/step - loss: 2.1551\n","Epoch 20/20\n","8/8 [==============================] - 1s 70ms/step - loss: 2.1501\n"]}]},{"cell_type":"code","source":["names_gen_raw = generate_text(names_model, names_chars, \"Andrea\\n\", temperature=0.2)"],"metadata":{"id":"lK37Nc3yu3x3","executionInfo":{"status":"ok","timestamp":1695988783516,"user_tz":0,"elapsed":8558,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def print_names_info(generated, train_set):\n","    names_gen = generated.split(\"\\n\")\n","\n","    unique_names = list(set(names_gen))\n","\n","    print(f\"Generated {len(names_gen)} names ({len(unique_names)} unique)\")\n","\n","    novel_names = [name for name in unique_names if not (name in train_set)]\n","\n","    print(f\"Generated {len(novel_names)} new names\")\n","    print(novel_names[:20])\n","\n","print_names_info(names_gen_raw, names_raw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ItkqRQmAw8ww","executionInfo":{"status":"ok","timestamp":1695988834221,"user_tz":0,"elapsed":13,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"63c306e0-3b8f-457b-811a-5563cfdfce28"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 154 names (102 unique)\n","Generated 53 new names\n","['Saberta', 'Marise', 'Annelle', 'Sabelle', 'Annella', 'Maritta', 'Beris', 'Berrin', 'Gerista', 'Sheris', 'Terie', 'Arissa', 'Sharina', 'Alerine', 'Jonne', 'Anelle', 'Gerina', 'Jorina', 'Sheria', 'Telly']\n"]}]},{"cell_type":"markdown","source":["##Question 3\n","The size of the model can make a difference when it comes to performance. Create a new model that has twice the number of hidden units as the previous model and double the size of the embeddings. How does the performance change? What happens if you decrease these parameters?"],"metadata":{"id":"64h0OI83sXJE"}},{"cell_type":"code","source":["# Your solution here\n","# 512-dimensional embeddings, 2048 RNN units\n","names_model2, names_chars2 = create_model(names_raw, epochs=20, embedding_dim=256*2, rnn_units=1024*2)"],"metadata":{"id":"Myk6_IUitYjk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695988377827,"user_tz":0,"elapsed":63518,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"ce8d48bb-3989-496b-c9f0-3d2b317fdb5c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["55 unique characters\n","Epoch 1/20\n","8/8 [==============================] - 4s 277ms/step - loss: 6.7158\n","Epoch 2/20\n","8/8 [==============================] - 3s 445ms/step - loss: 3.7908\n","Epoch 3/20\n","8/8 [==============================] - 2s 243ms/step - loss: 3.2994\n","Epoch 4/20\n","8/8 [==============================] - 2s 245ms/step - loss: 2.7795\n","Epoch 5/20\n","8/8 [==============================] - 5s 640ms/step - loss: 2.5164\n","Epoch 6/20\n","8/8 [==============================] - 2s 245ms/step - loss: 2.3883\n","Epoch 7/20\n","8/8 [==============================] - 5s 729ms/step - loss: 2.3185\n","Epoch 8/20\n","8/8 [==============================] - 2s 248ms/step - loss: 2.2738\n","Epoch 9/20\n","8/8 [==============================] - 3s 389ms/step - loss: 2.2423\n","Epoch 10/20\n","8/8 [==============================] - 2s 312ms/step - loss: 2.2155\n","Epoch 11/20\n","8/8 [==============================] - 2s 252ms/step - loss: 2.1924\n","Epoch 12/20\n","8/8 [==============================] - 5s 728ms/step - loss: 2.1745\n","Epoch 13/20\n","8/8 [==============================] - 2s 277ms/step - loss: 2.1590\n","Epoch 14/20\n","8/8 [==============================] - 2s 256ms/step - loss: 2.1479\n","Epoch 15/20\n","8/8 [==============================] - 3s 433ms/step - loss: 2.1345\n","Epoch 16/20\n","8/8 [==============================] - 2s 253ms/step - loss: 2.1243\n","Epoch 17/20\n","8/8 [==============================] - 5s 654ms/step - loss: 2.1086\n","Epoch 18/20\n","8/8 [==============================] - 2s 248ms/step - loss: 2.0980\n","Epoch 19/20\n","8/8 [==============================] - 2s 253ms/step - loss: 2.0830\n","Epoch 20/20\n","8/8 [==============================] - 2s 256ms/step - loss: 2.0721\n"]}]},{"cell_type":"code","source":["names_gen_raw = generate_text(names_model2, names_chars2, \"Andrea\\n\", temperature=0.2)\n","print_names_info(names_gen_raw, names_raw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fw4u2dqJx-wh","executionInfo":{"status":"ok","timestamp":1695988877322,"user_tz":0,"elapsed":8571,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"94c2e84a-ddba-4326-d368-7e69b3e620b9"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 149 names (80 unique)\n","Generated 37 new names\n","['Derina', 'Merisa', 'Annelle', 'Annella', 'Josine', 'Jelina', 'Andelina', 'Charie', 'Andelin', 'Anelle', 'Carise', 'Daris', 'Sheria', 'Cherista', 'Darina', 'Anda', 'Darila', 'Andelle', 'Marella', 'Andeline']\n"]}]},{"cell_type":"code","source":["# 128-dimensional embeddings, 512 RNN units\n","names_model3, names_chars3 = create_model(names_raw, epochs=20, embedding_dim=256//2, rnn_units=1024//2)\n","\n","names_gen_raw = generate_text(names_model3, names_chars3, \"Andrea\\n\", temperature=0.2)\n","print_names_info(names_gen_raw, names_raw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5qyW6822rt_","executionInfo":{"status":"ok","timestamp":1695989642975,"user_tz":0,"elapsed":25394,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"48e96927-c69e-4e90-b5ff-b22949cf5676"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["55 unique characters\n","Epoch 1/20\n","8/8 [==============================] - 5s 140ms/step - loss: 3.8251\n","Epoch 2/20\n","8/8 [==============================] - 1s 44ms/step - loss: 3.5619\n","Epoch 3/20\n","8/8 [==============================] - 0s 34ms/step - loss: 3.2624\n","Epoch 4/20\n","8/8 [==============================] - 1s 33ms/step - loss: 3.1276\n","Epoch 5/20\n","8/8 [==============================] - 0s 30ms/step - loss: 3.0782\n","Epoch 6/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.9794\n","Epoch 7/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.8427\n","Epoch 8/20\n","8/8 [==============================] - 0s 36ms/step - loss: 2.6731\n","Epoch 9/20\n","8/8 [==============================] - 0s 36ms/step - loss: 2.5296\n","Epoch 10/20\n","8/8 [==============================] - 1s 57ms/step - loss: 2.4407\n","Epoch 11/20\n","8/8 [==============================] - 0s 30ms/step - loss: 2.3903\n","Epoch 12/20\n","8/8 [==============================] - 0s 30ms/step - loss: 2.3557\n","Epoch 13/20\n","8/8 [==============================] - 0s 30ms/step - loss: 2.3304\n","Epoch 14/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.3078\n","Epoch 15/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.2828\n","Epoch 16/20\n","8/8 [==============================] - 0s 30ms/step - loss: 2.2669\n","Epoch 17/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.2537\n","Epoch 18/20\n","8/8 [==============================] - 0s 29ms/step - loss: 2.2390\n","Epoch 19/20\n","8/8 [==============================] - 0s 31ms/step - loss: 2.2258\n","Epoch 20/20\n","8/8 [==============================] - 0s 32ms/step - loss: 2.2164\n","Generated 155 names (76 unique)\n","Generated 36 new names\n","['Berine', 'Cardie', 'Marrin', 'Derina', 'Lanne', 'Tarie', 'Sharis', 'Sharin', 'Danne', 'Marista', 'Sherin', 'Marrie', 'Carche', 'Brista', 'Derin', 'Lerin', 'Cariste', 'Alben', 'Alenna', 'Darina']\n"]}]},{"cell_type":"markdown","source":["## Question 4\n","Transformer large language models can also generate text. The following code imports a pretrained GPT-2 model from Huggingface's Transformer library. This model can then be used directly to generate text, given a prompt as context. Alter the prompt to have the transformer model (GPT-2) generate an engaging story beginning using one of the following story starters:\n","\n","\n","*   It was the day the moon fell.\n","*   Am I in heaven?  What happened to me?\n","*   Wandering through the graveyard it felt like something was watching me.\n","*   Three of us.  We were the only ones left, the only ones to make it to the island.\n","\n","There are several different methods to choose from to generate the text (as seen in the commented out lines below). Try out the different methods and play with the parameters. This [blogpost](https://huggingface.co/blog/how-to-generate) explains their differences.\n","\n","Which method has the best performance?\n","\n","Can GPT-2 generate Shakespere?"],"metadata":{"id":"2FI-1ldhn0SE"}},{"cell_type":"code","source":["# Uncomment if transformers is not installed\n","!pip install transformers"],"metadata":{"id":"GE-xTvvYvHAy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695988980585,"user_tz":0,"elapsed":14484,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"3fb55384-8ddb-4091-de60-224183a4a3a1"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"]}]},{"cell_type":"code","source":["# Do not modify this code\n","# https://huggingface.co/docs/transformers/main_classes/text_generation\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","prompt = \"Today I believe we can finally\"\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(input_ids, max_length=100) # Greedy search\n","#outputs = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=3, early_stopping=True) # Beam search\n","#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=0, temperature=0.7) # Sampling\n","#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50) # Top-k\n","#outputs = model.generate(input_ids, do_sample=True, max_length=100, top_k=50, top_p=0.92) # Top-p\n","\n","tokenizer.batch_decode(outputs, skip_special_tokens=True)"],"metadata":{"id":"2_ZMgywnnziH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695990032892,"user_tz":0,"elapsed":14105,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"7047760b-26fe-49e8-edbb-368cd6510c3c"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can make a difference in the lives of the people of the United States of America.\\n\\n']"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# Your solution here\n","def generate_from_prompt(prompt, model, mode=\"greedy\", max_length=100):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    if mode == \"greedy\":\n","        args = {}\n","    elif mode == \"beam\":\n","        args = {\"num_beams\": 5, \"no_repeat_ngram_size\": 3, \"early_stopping\": True}\n","    elif mode == \"sampling\":\n","        args = {\"do_sample\": True, \"top_k\": 0, \"temperature\": 0.7}\n","    elif mode == \"topk\":\n","        args = {\"do_sample\": True, \"top_k\": 50}\n","    elif mode == \"topp\":\n","        args = {\"do_sample\": True, \"top_k\": 50, \"top-p\": 0.92}\n","    else:\n","        return \"\"\n","\n","    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, **args)\n","    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"],"metadata":{"id":"hnH32YYKraN4","executionInfo":{"status":"ok","timestamp":1695990802820,"user_tz":0,"elapsed":315,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["print(generate_from_prompt(\"Today I believe we can finally\", model)[0]) # Greedy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-Ez0vku4xSO","executionInfo":{"status":"ok","timestamp":1695990739744,"user_tz":0,"elapsed":5760,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"e1e2afc7-0421-42ae-c15a-cba8fac877f7"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\n","\n","I believe that we can make a difference in the lives of the people of the United States of America.\n","\n","I believe that we can make a difference in the lives of the people of the United States of America.\n","\n","I believe that we can make a difference in the lives of the people of the United States of America.\n","\n","\n"]}]},{"cell_type":"code","source":["print(generate_from_prompt(\"It was the day the moon fell.\", model, mode=\"beam\")[0]) # Beam search"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1v8fmiK15L_","executionInfo":{"status":"ok","timestamp":1695990752905,"user_tz":0,"elapsed":10732,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"bb6ec3f4-2965-4bfd-8ebd-a20051069dd2"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["It was the day the moon fell.\n","\n","\"It was a beautiful day,\" she said. \"It was beautiful. It was beautiful.\"\n"]}]},{"cell_type":"code","source":["print(generate_from_prompt(\"Wandering through the graveyard it felt like something was watching me.\", model, mode=\"sampling\")[0]) # Sampling"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncCNLYjE16mm","executionInfo":{"status":"ok","timestamp":1695990762095,"user_tz":0,"elapsed":5227,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"305860f4-d015-415b-ec5f-abfc63c1cc09"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Wandering through the graveyard it felt like something was watching me.\n","I looked around. There were the two guards that I recognized. One was unarmed, with a pistol, the other a single-handed assault rifle.\n","They both looked at me with an expression of disapproval.\n","\"I don't know what to say,\" I said.\n","The guard said nothing. He glanced at me. \"You're not doing anything wrong.\"\n","I looked back.\n","I didn't have anything to\n"]}]},{"cell_type":"code","source":["print(generate_from_prompt(\"Am I in heaven? What happened to me?\", model, mode=\"topk\")[0]) # Top-k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJ-hueHj19xb","executionInfo":{"status":"ok","timestamp":1695990328924,"user_tz":0,"elapsed":8582,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"de4d9266-663e-4eb2-e015-e9606c489e4f"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Am I in heaven? What happened to me? Oh, God, please, if you hear me, be sure to pray.\" I bowed in prayer. I was about to pray another minute but I was no longer willing to pray because I realized my answer was not going very well and I was now standing at a cross and not feeling well. What seemed to me to be one of those \"things\", was more than a little disturbing. Now the prayer changed. What I saw was a world wide\n"]}]},{"cell_type":"code","source":["print(generate_from_prompt(\"Wandering through the graveyard it felt like something was watching me.\", model, mode=\"topp\")[0]) # Top-p"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bzoJER92AG4","executionInfo":{"status":"ok","timestamp":1695990386464,"user_tz":0,"elapsed":5225,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"5e91a5d5-0b2e-4e33-d8ad-3df3162a0822"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Wandering through the graveyard it felt like something was watching me. But when I was finished, I turned around and looked for where to find the keys. No one was there.\n","\n","The man in the black hood pulled a lever and pulled a button. Then, he gave me a hug. I started to run for the door. As I passed by the other guy, I saw the one in the hood that was holding the key. The boy in the black hood started running as fast as\n"]}]},{"cell_type":"code","source":["print(generate_from_prompt(shakespeare[:256], model, mode=\"sampling\")[0]) # Sampling"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9J2XMeWO3iDq","executionInfo":{"status":"ok","timestamp":1695990487696,"user_tz":0,"elapsed":3346,"user":{"displayName":"Andrea Terenziani","userId":"16483460136645320142"}},"outputId":"5234f8f7-6d38-4e5c-a785-d4f73c4ea7d9"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","\n","I refuse to give up the fight for the people.\n","\n","First Citizen:\n","\n","You must\n"]}]}]}