{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gb8JZdRCj6L"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 3\n",
        "In today's lab, we will be working with logistic regression and part-of-speech tagging, and word embeddings.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4Pb6jbR0_j"
      },
      "source": [
        "## Extracting numerical features from text\n",
        "Machine learning algorithms generally only accept numerical input, meaning that we must represent all features numerically. For example, to classify a single sentence, we might pass a classifier a list of word counts in that sentence, or a list of `True` and `False` values (which have numerical values of 1 and 0, respectively), representing the presence or absence of particular words.\n",
        "\n",
        "[Scikit-learn](https://scikit-learn.org/stable/) is a popular machine learning library for Python that implements a wide variety of machine learning algorithms, including naive Bayesian and logistic regression. It also offers a convenient way to extract numerical features from text, for example with the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class. `CountVectorizer` is used to generates feature vectors containing character or word n-gram counts for any n within a given range (e.g., `ngram_range=(2, 2)` for only bigrams, or `ngram_range(1, 3)` for unigrams, bigrams and trigrams)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bMHZMO5TNbd",
        "outputId": "1a1cdc56-0d41-4759-b231-3cffb50f187d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\andre\\OneDrive - Reykjavik University\\Code\\NLP\\Lab 3\\T_725_Lab03.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive%20-%20Reykjavik%20University/Code/NLP/Lab%203/T_725_Lab03.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive%20-%20Reykjavik%20University/Code/NLP/Lab%203/T_725_Lab03.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Initialize a vectorizer that counts word bigrams\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive%20-%20Reykjavik%20University/Code/NLP/Lab%203/T_725_Lab03.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)) \u001b[39m#(2,2) means that we want N-grams with N=2 (from 2 to 2)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m _object_dtype_isnan\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    scikit-learn.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\fixes.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\stats\\__init__.py:612\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    611\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_morestats\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 612\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_multicomp\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    613\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_binomtest\u001b[39;00m \u001b[39mimport\u001b[39;00m binomtest\n\u001b[0;32m    614\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_binned_statistic\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\stats\\_multicomp.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimize\u001b[39;00m \u001b[39mimport\u001b[39;00m minimize_scalar\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_common\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfidenceInterval\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_qmc\u001b[39;00m \u001b[39mimport\u001b[39;00m check_random_state\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m _var\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\stats\\_qmc.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m \u001b[39mimport\u001b[39;00m distance, Voronoi\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m gammainc\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_sobol\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     _initialize_v, _cscramble, _fill_p_cumulative, _draw, _fast_forward,\n\u001b[0;32m     33\u001b[0m     _categorize, _MAXDIM\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_qmc_cy\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     _cy_wrapper_centered_discrepancy,\n\u001b[0;32m     37\u001b[0m     _cy_wrapper_wrap_around_discrepancy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     _cy_van_der_corput,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     46\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiscrepancy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mupdate_discrepancy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     47\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mQMCEngine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSobol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHalton\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLatinHypercube\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPoissonDisk\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mMultinomialQMC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMultivariateNormalQMC\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize a vectorizer that counts word bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2)) #(2,2) means that we want N-grams with N=2 (from 2 to 2)\n",
        "\n",
        "# Count all bigrams in the sentences and create a feature vector\n",
        "sentences = [\"It was the best of times, it was the worst of times,\",\n",
        "             \"it was the age of wisdom, it was the age of foolishness,\"]\n",
        "\n",
        "vector = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Bigrams:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nFeatures:\")\n",
        "print(vector.toarray()) # One row for each sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-9LuaLX_Yw"
      },
      "source": [
        "Here, `vectorizer` created a matrix with 13 columns (one for each bigram) and two rows (one for each sentence). Each row consists of bigram counts for the corresponding sentence. For example, the first sentence has the bigram counts `[0 1 2 0 2 0 0 1 1 1 2 0 1]`, which means that it contains 0 instances of \"age of\", 1 instance of \"best of\", two instances of \"it was\", and so on (we can see which column represents which bigram with `vecorizer.get_feature_names_out()`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Jb9cBxhTMh"
      },
      "source": [
        "## Creating training and test sets\n",
        "Scikit-learn lets us quickly split data into training and test sets with the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. Note that by convention, examples are generally denoted with a capital X while labels are denoted with a lowercase y. Let's create a training and test set for the subjectivity corpus from the NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YbXr6-KhYDR",
        "outputId": "e65714ee-6745-4306-90ac-ba87b19c6da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: 10000 | y shape: 10000\n",
            "Unigrams: ... ['answering' 'answers' 'antagonism' 'antagonized' 'ante' 'antes'\n",
            " 'anthology' 'anthony' 'anthropology' 'anthropomorphic' 'anti' 'antic'\n",
            " 'antichrist' 'anticipate' 'anticipated' 'anticipating' 'antics'\n",
            " 'antidote' 'antidotes' 'antigone' 'antique' 'antiseptic' 'antitrust'\n",
            " 'antlers' 'antoine' 'anton' 'antonia' 'antonin' 'antonio' 'ants' 'antsy'\n",
            " 'antwone' 'anxieties' 'anxiety' 'anxious' 'any' 'anybody' 'anymore'\n",
            " 'anyone' 'anything' 'anytime' 'anyway' 'anywhere' 'ao' 'apart'\n",
            " 'apartheid' 'apartment' 'apartments' 'apasionamiento' 'ape' 'apenas'\n",
            " 'apes' 'apesar' 'aplomb' 'apocalypse' 'apocalyptic' 'apolitical'\n",
            " 'apolitically' 'apologetics' 'apology' 'appalling' 'apparatus' 'apparel'\n",
            " 'apparent' 'apparently' 'appeal' 'appealing' 'appealingly' 'appear'\n",
            " 'appearance' 'appearances' 'appeared' 'appearing' 'appears' 'appease'\n",
            " 'appeasing' 'appetite' 'appetites' 'applaud' 'applauded' 'applebee'\n",
            " 'applegate' 'appliance' 'appliances' 'application' 'applied' 'applies'\n",
            " 'apply' 'applying' 'appointed' 'appointment' 'appraisal' 'appreciate'\n",
            " 'appreciated' 'appreciates' 'appreciating' 'appreciative' 'apprehend'\n",
            " 'apprentice' 'approach'] ...\n",
            "Features: ...\n",
            "  (0, 18614)\t2\n",
            "  (0, 12182)\t1\n",
            "  (0, 1833)\t1\n",
            "  (0, 9236)\t1\n",
            "  (0, 13421)\t1\n",
            "  (0, 20439)\t1\n",
            "  (0, 20833)\t1\n",
            "  (0, 2347)\t1\n",
            "  (0, 12352)\t1\n",
            "  (0, 16034)\t1\n",
            "  (0, 1389)\t1\n",
            "  (0, 18844)\t1\n",
            "  (0, 16123)\t1\n",
            "  (0, 3016)\t1\n",
            "  (0, 7469)\t1\n",
            "  (0, 8963)\t1\n",
            "  (1, 18614)\t2\n",
            "  (1, 9236)\t1\n",
            "  (1, 7469)\t1\n",
            "  (1, 6037)\t1\n",
            "  (1, 8923)\t1\n",
            "  (1, 14545)\t1\n",
            "  (1, 918)\t2\n",
            "  (1, 16770)\t1\n",
            "  (1, 3111)\t1\n",
            "  :\t:\n",
            "  (9999, 918)\t1\n",
            "  (9999, 12860)\t1\n",
            "  (9999, 18613)\t1\n",
            "  (9999, 12928)\t1\n",
            "  (9999, 9818)\t1\n",
            "  (9999, 18341)\t1\n",
            "  (9999, 20608)\t1\n",
            "  (9999, 15762)\t1\n",
            "  (9999, 3537)\t1\n",
            "  (9999, 15621)\t1\n",
            "  (9999, 6422)\t1\n",
            "  (9999, 19334)\t1\n",
            "  (9999, 6570)\t1\n",
            "  (9999, 9480)\t1\n",
            "  (9999, 19810)\t1\n",
            "  (9999, 20121)\t1\n",
            "  (9999, 4829)\t1\n",
            "  (9999, 9118)\t1\n",
            "  (9999, 18690)\t1\n",
            "  (9999, 7153)\t1\n",
            "  (9999, 8672)\t1\n",
            "  (9999, 16473)\t1\n",
            "  (9999, 18632)\t1\n",
            "  (9999, 10226)\t1\n",
            "  (9999, 9560)\t1\n",
            "...\n",
            "(8000, 20909)\n",
            "(2000, 20909)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import subjectivity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download the subjectivity corpus and get the sentences for each category\n",
        "nltk.download('subjectivity', quiet=True)\n",
        "\n",
        "obj_fileids = subjectivity.fileids('obj')\n",
        "subj_fileids = subjectivity.fileids('subj')\n",
        "\n",
        "# Let's get the untokenized sentences from each category\n",
        "obj_sentences = subjectivity.raw(obj_fileids).splitlines()\n",
        "subj_sentences = subjectivity.raw(subj_fileids).splitlines()\n",
        "\n",
        "X = obj_sentences + subj_sentences\n",
        "y = ['obj'] * 5000 + ['subj'] * 5000\n",
        "\n",
        "print(f\"X shape: {len(X)} | y shape: {len(y)}\")\n",
        "\n",
        "# Create a word unigram count vectorizer and generate the feature vectors\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "print(\"Unigrams: ...\", vectorizer.get_feature_names_out()[1000:1100], \"...\")\n",
        "print(f\"Features: ...\\n{X_vectorized}\\n...\")\n",
        "\n",
        "# Create a training and test set (80%/20% split). This function always shuffles\n",
        "# the examples before making the split, but we can make sure that it always\n",
        "# shuffles them the same way by specifying a specific random_state value.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "#print(y_train.shape)\n",
        "#print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCTmsvXzgY-C"
      },
      "source": [
        "## Logistic regression\n",
        "We can create a logistic regression classifier with the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGffjsuFghoi",
        "outputId": "26f31694-ffbd-4a60-9ab2-a8ffa91866c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.2%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "subj_clf = LogisticRegression(solver='liblinear')\n",
        "subj_clf.fit(X_train, y_train)  # Train the model\n",
        "score = subj_clf.score(X_test, y_test)  # Evaluate the model on the test set\n",
        "print(f\"Accuracy: {score:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZEznKZAuj1-"
      },
      "source": [
        "Our logistic regression classifier obtains an accuracy of 90.2%, which is quite a bit higher than the accuracy obtained by NLTK's naive Bayes classifier in a previous lab.\n",
        "\n",
        "Once the classifier is trained, we can use it to classify new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ev9cbxGunWk",
        "outputId": "d618d48b-057d-4ec1-b079-2dced888f9ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['obj', 'subj'], dtype='<U4')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_sentences = [\n",
        "  \"Monty Python's Flying Circus, the British comedy group which gained fame via\\\n",
        "   BBC-TV, send-up Arthurian legend, performed in whimsical fashion with Graham\\\n",
        "   Chapman an effective straight man as King Arthur.\",\n",
        "  \"The funniest movie of 1975 and probably the silliest movie ever made.\"\n",
        "]\n",
        "\n",
        "features = vectorizer.transform(example_sentences)\n",
        "subj_clf.predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKiN7bhwfk0j"
      },
      "source": [
        "## Pipelines\n",
        "Instead of having to call `vectorizer.transform()` every time we use the classifier, we can create a `Pipeline` that automatically extracts features for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMuh7usegvTw",
        "outputId": "91bdfbbe-1bfa-4f5f-fa68-1be66e896b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.2%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['obj', 'subj'], dtype='<U4')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Each element in the pipeline simply needs to implement a \"fit\" method and \"transform\" method\n",
        "# so the names in the 1st argument of each tuple are completely arbitrary\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# The feature vectors are automatically created\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "score = pipeline.score(X_test, y_test)\n",
        "print(\"Accuracy: {:.1%}\".format(score))\n",
        "\n",
        "pipeline.predict(example_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8AqtdrsdfZ0"
      },
      "source": [
        "## Creating word embeddings\n",
        "[Gensim](https://radimrehurek.com/gensim/) is a Python library that makes it easy to generate and work with word embeddings.\n",
        "\n",
        "Let's start by supressing some warnings from Gensim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "R7ElxTOtl6UQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress some warnings from Gensim about deprecated functions\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs583ebKoiXK"
      },
      "source": [
        "Now, let's create word2vec embeddings for NLTK's movie review corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hLvQMJDrdvdW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('movie_reviews', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "sents = movie_reviews.sents()\n",
        "movie_embeddings = Word2Vec(sents, epochs=1, min_count=5, vector_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHF4ReN-DVr3"
      },
      "source": [
        "What does the vector for *actor* look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpOAPLLktUmz",
        "outputId": "422f1687-57be-48dd-cccf-9075d5077481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.3687924e-01,  1.2254957e-01, -1.6911164e-01, -1.0183899e-01,\n",
              "       -3.1075522e-01, -3.1336010e-01,  9.4830650e-01,  6.5819436e-01,\n",
              "       -9.8458105e-01, -4.2445722e-01,  5.0743598e-02, -7.3398995e-01,\n",
              "        5.8754784e-01,  4.7781390e-01, -3.6803344e-01,  3.5719073e-01,\n",
              "        5.0480533e-01,  2.6331025e-01, -1.1753895e+00, -7.1344900e-01,\n",
              "        2.9667971e-01,  6.3921458e-01,  9.5206887e-01, -1.4922971e-01,\n",
              "        4.3971506e-01,  3.4288752e-01,  3.5215467e-02, -1.0082237e-03,\n",
              "       -5.3435469e-01, -1.3530084e-03, -5.6954108e-02, -6.4841348e-01,\n",
              "        1.7686807e-01, -1.6867687e-01, -2.8758591e-01,  1.4085408e-01,\n",
              "        5.3365463e-01,  5.6468297e-02,  1.4444064e-01, -3.0620471e-01,\n",
              "        4.6456853e-01, -3.9902335e-01,  3.3807570e-01,  2.6367715e-01,\n",
              "        1.2789340e+00,  2.7660515e-02, -1.2256439e-01, -6.5408212e-01,\n",
              "        4.4967675e-01,  3.3568975e-01], dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_embeddings.wv['actor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEfsQ7TJCutn"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 8:30 AM, Monday September 18th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Zayf9rC2cP"
      },
      "source": [
        "## Question 1\n",
        "The NLTK includes a copy of the *Universal Declaration of Human Rights* (UDHR) in over 300 languages, including Icelandic, Norwegian, Swedish, Danish, Finnish and Faroese.\n",
        "\n",
        "Create a `Pipeline` with a `CountVectorizer` and a `LogisticRegression` classifier that satisfies the following requirements:\n",
        "\n",
        "The `CountVectorizer` should:\n",
        "* Create character-level n-grams.\n",
        "* Generate unigram, bigram and trigram counts.\n",
        "\n",
        "The `LogisticRegression` classifier should:\n",
        "* Use the `liblinear` solver.\n",
        "\n",
        "Refer to Scikit-learn's reference for the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for information on possible parameters.\n",
        "\n",
        "Once you've created the pipeline, train it using the `train_udhr(pipeline)` function below, which returns the test examples and labels (and should not be modified). Report the accuracy of the classifier, and try making predictions on a few sentences from these languages, for example from Wikipedia ([is](https://is.wikipedia.org/wiki/Fors%C3%AD%C3%B0a), [no](https://no.wikipedia.org/wiki/Portal:Forside), [se](https://sv.wikipedia.org/wiki/Portal:Huvudsida), [da](https://da.wikipedia.org/wiki/Forside), [fi](https://fi.wikipedia.org/wiki/Wikipedia:Etusivu), [fo](https://fo.wikipedia.org/wiki/Fors%C3%AD%C3%B0a)). One sentence from each language is enough. Does the classifier perform as well as you would expect, given the reported accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Gypk375K55Zj"
      },
      "outputs": [],
      "source": [
        "# Don't change anything in this code cell\n",
        "import random\n",
        "from nltk.corpus import udhr\n",
        "nltk.download('udhr', quiet=True)\n",
        "\n",
        "def train_udhr(pipeline):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # The UDHR is quite small, so let's create 1,000 \"fake\" sentences in each\n",
        "    # language by randomly stringing together 3-15 words.\n",
        "    for lang in languages:\n",
        "        words = udhr.words(lang)\n",
        "        sents = [\" \".join(random.choices(words, k=random.randint(3, 15))) for x in range(1000)]\n",
        "        X.extend(sents)\n",
        "        y += [lang] * len(sents)\n",
        "\n",
        "    print(len(X))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    return X_test, y_test\n",
        "\n",
        "languages = ['Icelandic_Yslenska-Latin1',\n",
        "             'Norwegian-Latin1',\n",
        "             'Swedish_Svenska-Latin1',\n",
        "             'Danish_Dansk-Latin1',\n",
        "             'Finnish_Suomi-Latin1',\n",
        "             'Faroese-Latin1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-sM02NcDrZb",
        "outputId": "3f102b78-e705-49c3-c929-7355ac71a191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6000\n",
            "Accuracy: 96.2%\n"
          ]
        }
      ],
      "source": [
        "# Your solution here\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1, 3), analyzer=\"char\")),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "X_test, y_test = train_udhr(pipeline)\n",
        "score = pipeline.score(X_test, y_test)\n",
        "print(\"Accuracy: {:.1%}\".format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwetpCxTCwXB"
      },
      "source": [
        "## Question 2\n",
        "The logistic regression classifier below tries to determine which of the following tags should be assigned to a given word:\n",
        "* **NP** (proper nouns, singular),\n",
        "* **NP\\$** (proper nouns, singular and possessive),\n",
        "* **VBG** (verbs, present participle) or\n",
        "* **VBD** (verbs, past tense).\n",
        "\n",
        "The classifier makes its determination solely on characteristics of the word itself and does not make use of any contextual features. The function `extract_features(word)` extracts a list of numerical features from each word, currently only the length of a word and whether or not it ends with \"r\". Using these features, the classifier obtains an accuracy of 37.1%, which is quite poor. Replace the features that the `exctract_features()` function generates with your own. Use Python's [string methods](https://docs.python.org/3/library/stdtypes.html#string-methods) to generate the features, and try to get at least 99% accuracy.\n",
        "\n",
        "**Remember**: each feature must be numerical (or `True`/`False`), and don't forget to add a comma after each feature in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-be0psqueELC",
        "outputId": "507fb8b5-f0b1-4919-d655-c6adaae5871b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sentences: 51,606\n",
            "Test sentences: 5,734\n"
          ]
        }
      ],
      "source": [
        "# Don't change anything in this code cell\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def get_brown_tags(tag):\n",
        "  return sorted({w for s in brown_train for w, t in s if t == tag})\n",
        "\n",
        "def train_model():\n",
        "  # Create the training set\n",
        "  word_list = [word for tag_words in words for word in tag_words]\n",
        "  X = [extract_features(word) for word in word_list]\n",
        "  y = [tag for tag, tag_words in zip(tags, words) for word in tag_words]\n",
        "\n",
        "  # Train and evaluate the classifier\n",
        "  log_clf = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "  log_clf.fit(X, y)\n",
        "  print(\"Accuracy: {:.1%}\".format(log_clf.score(X, y)))\n",
        "\n",
        "  # Print the accuracy for each tag\n",
        "  predictions = log_clf.predict(X)\n",
        "  errors = defaultdict(list)\n",
        "  for word, example, label, prediction in zip(word_list, X, y, predictions):\n",
        "    if label != prediction:\n",
        "      errors[label].append(word)\n",
        "\n",
        "  print(\"\\nAccuracy and first 10 errors per tag:\")\n",
        "  for tag, tag_words in zip(tags, words):\n",
        "    error_words = errors[tag]\n",
        "    num_total = len(tag_words)\n",
        "    num_correct = num_total - len(error_words)\n",
        "    ratio = num_correct / num_total\n",
        "    print(\"{:>3} {:,}/{:,} ({:.1%}) {}\".format(tag, num_correct, num_total, ratio,\n",
        "                                              \", \".join(error_words[:10])))\n",
        "\n",
        "nltk.download('brown', quiet=True)\n",
        "# Download and prepare the Brown corpus for training and testing\n",
        "brown_train, brown_test = train_test_split(brown.tagged_sents(),\n",
        "                                           test_size=0.1,\n",
        "                                           random_state=42)\n",
        "\n",
        "print(\"Training sentences: {:,}\".format(len(brown_train)))\n",
        "print(\"Test sentences: {:,}\".format(len(brown_test)))\n",
        "\n",
        "# Get 1,000 examples of each tag\n",
        "tags = ['NP', 'NP$', 'VBG', 'VBD']\n",
        "random.seed(42)\n",
        "words = [random.sample(get_brown_tags(tag), 1000) for tag in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwWCtDcyypEE",
        "outputId": "010ad905-f0dc-4d0e-ccbf-f3bf283c10e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Quint', 'Bristol', 'Amis', 'Steinkerque', 'Foggia', 'Elsie', 'Dorr', 'Carnegie', 'Squibb', 'Bowman', 'Ryerson', 'Steele', 'Mrs.', 'Bizerte', 'Pak', 'Keo', 'Anthony', 'Anita', 'Boismassif', 'Dodd', 'Durocher', 'Maureen', 'Pedro', 'Anchorite', 'Neversink', 'Daniels', 'Sienkiewicz', 'Reese', 'Self', 'Mousie', 'Kelsey', 'Dominique', 'Lauri', 'Pabor', 'Foster', 'Virdon', 'Acey', 'Tahoe', 'Vicenza', 'Christ', 'Scott', 'Kerrville', 'Harnack', 'Fosdick', 'Cherokee', 'Dimitri', 'Taui', 'Hanover-Misty', 'Bowdoin', 'Bodin', 'Ierulli', 'Bonnie', 'Hildy', 'Yoneda', 'Hattiesburg', 'Pendleton', 'Fenster', 'Vidal', 'Astarte', 'Sonora', 'Leyden', 'Moloch', 'Burr', 'IBM', 'Bernhard', 'NATO', 'Garibaldi', 'Whiting', 'Poupin', 'Place', 'Hirey', 'Odom', 'Curtiss', 'Seurat', 'Beckett', 'Attakapas', 'Riverside', 'Dronk', 'Thurman', 'Gagarin', 'Berra', 'mid-April', 'Dussa', 'Boun', 'Ifni', 'Foss', 'Leet', 'Puddingstone', 'Wilmington', 'Holabird', 'Cinemactor', 'Horton', 'Heron', 'Dequindre', 'Rotelli', 'Field', 'Semenov', 'Sam', 'Raynal', 'Bel', 'Pettersson', 'Prudence', 'Colo.', 'Moffett', 'Song', 'Eloise', 'Claire', 'Linden', 'Ida', 'Fishkill', 'Quyne', 'Sartoris', 'Neal', 'Dollar-Britten', 'Sanchez', 'Grosvenor', 'Xenia', 'Tex.', 'Time-Mynah', 'Balenciaga', 'Dubois', 'Weigand', 'Antigone', 'Vesole', 'Grafton', 'Jimenez', 'Filippo', 'Baylor', 'Dexedrine', 'Niven', 'Simca', 'Goyette', 'Dick', 'Rhine', 'Mars', 'Jaycee', 'Rak', 'Lew', 'Carwood', 'Ferdinand', 'Emmert', 'Stinky', 'Nevsky', 'Montaigne', 'Fay', 'Strasbourg', 'Osborne', 'Kitty', 'Ortega', 'Jerome', 'Hiss', 'Dogtown', 'Carlisle', 'McDaniel', 'Mar.', 'Blumberg', 'Sydney', 'Augustine', 'Bridges', 'Chateau', 'Post', 'Christiana', 'Turner', 'Sadie', 'Kerby', 'Pascataqua', 'Basie', 'Iraj', 'Ilona', 'Parvenu', 'Lolly', 'Mingus', 'Ernie', 'Nairobi', 'Affaire', 'Sacramento', 'Sizova', 'Browne', 'Salisbury', 'Momoyama', 'Sulcer', 'Fidel', 'Texas', 'Rabat', 'Harmony', 'Britain', 'Garnett', 'Kopstein', 'China', 'Leggett', 'ATP', 'Skyros', 'Sino-Soviet', 'February', 'Martinez', 'Tarrant', 'Cooke', 'McAuliffe', 'Brasstown', 'Pontius', 'Gene-Princess', 'Wycoff', 'Quick-Wate', 'Petrini', 'Dannehower', 'Huffman', 'Taruffi', 'Church', 'Monterey', 'Tolstoy', 'Minor', '400-401', 'Paula', 'Grossman', 'Malia', 'Aliah', 'Brittany', 'Hobart', 'Wilfred', 'Victoria', 'Gizenga', 'Edythe', 'Banfield', 'Ekstrohm', 'Nixon', 'Birdwhistell', 'Spada', 'Mahayana', 'Wallenstein', 'Bechhofer', 'Tao', 'Mitch', 'Teller', 'Buster', 'CSF', 'Rimini', 'Lucius', 'Mycenae', 'Clemence', 'Ferguson', 'Mille', 'Pernod', 'Keys', 'DiSimone', 'Monte', 'Switzer', 'Sophocles', 'Sawallisch', 'Dartmouth', 'Shooter', 'Gompachi', 'Jeremiah', 'Roy', 'Huck', 'Kruger', 'Melzi', 'LeSourd', 'Bundestag', 'Englishman', 'Dover', 'Bassi', 'Harbert', 'Almighty', 'P.-T.A.', 'Nancy', 'Dulles', 'P.D.I.', 'Dominic', 'Ada', 'Beirut', 'Shaw', 'Presbyterianism', 'Baptist', 'Dublin', 'Bean', 'Antarctica', 'Hackett', 'Beiderbecke', 'Mead', 'Edison', 'Fox', 'Rosenberg', 'Maggie', 'Dill', 'Callas', 'Sloane', 'Northampton', 'Occident', 'Lovejoy', 'Eliot', 'Traxel', 'Loveless', 'Vientiane', 'Jubal', 'Cubism', 'Bolingbroke', 'Bontempo', 'Rig-Veda', 'Knowlton', 'Hercules', 'Khan', 'Kahler', 'Lodley', 'Sonenberg', 'Bake', 'Ruger', 'Resnik', 'Rathbone', 'Bordner', 'Barnard', 'Johann', 'Solomon', 'Hardy', 'Vasady', 'Bricker', 'Enright', 'Curie', 'Cuban-American', 'Molotov', 'Laura', 'Carol', 'Coughlin', 'Lindy', 'Eppler', 'Bennett', 'Lamar', 'Vigreux', 'niger', 'Myron', 'Borak', 'BAM', 'Renfrew', 'Montreal', 'Winter', 'Ala.', 'Bohlen', 'Sutherland', 'Yvette', 'Eaton', 'Cliff', 'Jouvet', 'Magoun', 'MacPhail', 'Diffring', 'Jim', 'Bankhead', 'Claus', 'Ibrahim', 'ABC', 'Jackie', 'Feringa', 'Train', 'Treece', 'Len', 'Fritz', 'Key', 'Schwada', 'Sounder', 'Towsley', 'Nate', 'Robards', 'Simms', 'Mahzeer', 'Chen', 'Cruz', 'Gavin', 'Dnieper', 'Banjo', 'Oleg', 'Springfield', 'Morgart', 'Barnett', 'Strickland', 'Gorton', 'Balzac', 'B.B.C.', 'Ory', 'Luis', 'Masters', 'aerogenes', 'Miriam', 'Chico', 'Ballestre', 'McCafferty', 'Bert', 'Ziminska-Sygietynska', 'Cramer', 'Beaulieu', 'Parks', 'Beatrice', 'Russia', 'ESP', 'Johnston', 'Buffalo', 'Norell', 'Emile', 'Oldenburg', 'Park', 'Armour', 'Plato', 'Beta', 'Kellum', 'Richter-Haaser', 'Orvil', 'Nigeria', 'Meynell', 'Granny', 'Farnworth', 'Deadwood', 'Rossoff', 'Sierra', 'Gotterdammerung', 'Edw', 'Fernberger', 'Jean', 'Calcutta', 'Raphael', 'Gerald', 'Lerner', 'Grande-Bretagne', 'Sumner', 'Bellini', 'Adios-Trustful', 'Leverkuhn', 'Pocasset', 'Newsweek', 'Bottega', 'Belshazzar', 'Mondrian', 'Dickson', 'Max', 'Calude', 'Hegel', 'Beaver', 'Ellwood', 'Horn', 'Frisco', 'Child', 'Krumpp', 'Willis', 'Morrison', 'Serene', 'Ghana', 'Phillip', 'Reuther', 'Minerva', 'Adamson', 'Roosevelt', 'Walton', 'Nara', 'Georgetown', 'Rochdale', 'Bracken', 'Campbell', 'Bruegel', 'Brazilian', 'Stephane', 'Flint', 'Fred', 'Peony', 'Devol', 'Silesia', 'Harve', 'DeGroot', 'Saratoga', 'Protitch', 'anhemolyticus', 'Felicity', 'Maurice', 'Erlenmeyer', 'Yen', 'BG', 'Bock', 'Proudhon', 'Forrest', 'Aterman', 'Ab63711-r', 'Hammarskjold', 'Thomas', 'Calabria', 'Push-Pull', 'Faulkner', 'Steinberg', 'Lacy', 'N.Y.U.', 'Shabbat', 'Kirkwood', 'Nev.', 'Adler', 'Benjamin', 'Scandinavia', 'Chablis', 'Mudugno', 'Archimedes', 'Winchester', 'Hopkins', 'Orinoco', 'Nagamo', 'Cervantes', 'Kleist', 'Byronism', 'Asher', 'Glazer-Fine', 'Hokan', 'Unitarianism', 'Armstrong', 'Hewlett-Woodmere', 'Detroit', 'Sally', 'Ephesus', 'Rome', 'Boy-Marquita', 'Herald-Examiner', 'Tony', 'Nerien', 'Plutarch', 'Styka', 'Chekhov', 'Eckart', 'Cincinnati', 'Varani', 'Vivier', 'Conrad', 'Kamieniec', 'Americana', 'Cooperman', 'Halkett', 'Toscanini', 'Kalamazoo', 'Venice', 'Rostagno', 'Spofford', 'Enoch', 'Chorale', 'Tristan', 'Selena', 'Brenner', 'Indian', 'Arkansas', 'Lorenz', 'Dorado', 'Danville', 'Walt', 'Liebler', 'Heilman', 'Gill', 'Weaver', 'Ulyate', 'Drexel', 'Doris', 'Alwin', 'Ring', 'Cyprian', 'Jenny', 'Gustave', 'Foxx', 'Becket', 'Tiao', 'Fran', 'Helmut', 'Racie', 'McCullough', 'Jerusalem', 'Sabbath', 'Yahwe', 'Haestier', 'Andrei', 'Bruckner', 'Farrar', 'Cook', 'Oman', 'Fernand', 'Aristide', 'Breton', 'Paschall', 'Kooning', 'Haydon', 'Somay', 'Trikojus', 'Kraemer', 'Perle', 'McIver', 'Brumidi', 'Ireland', 'October', 'Cuban', 'Evangelicalism', 'Athabascan', 'Shea', 'Kowalski', 'A.I.D.', 'Meriwether', 'Vicolo', 'Monk', 'Sara', 'Sing', 'Stennis', 'St.', 'Rottger', 'Dalzell-Cousin', 'Hoffman', 'Knoxville', 'Bee-Hunter', 'Rockport', 'Habib', 'Pole', 'Gosson', 'Young', 'Burnside', 'Sinton', 'Gerhard', 'Mazeroski', 'Glimco', 'Julie', 'Guglielmo', 'Joel', 'Scolatti', 'Gator', 'Naples', 'Byrnes', 'Curry', 'Kenilworth', 'Rodney-Honor', 'Ibsen', 'S.', 'Strindberg', 'Comroe', 'Pike', 'Norberg', 'Geroge', 'Jossy', 'Murrow', 'Willings', '18th-Century', 'Gibson', 'Fujimoto', 'Deutsch', 'Klimt', 'Triandos', 'Olivia', 'Reverend', 'Grigori', 'Little', 'Lafayette', 'Ruth', 'Dietrich', 'McIntosh', 'Lovett', 'U.M.T.', 'Sprinkel', 'Cole', 'Riesman', 'Billings', 'Frenchman', 'Meg', 'Rockefeller', 'Princeton', 'Plath', 'Haney', 'Bois', 'Wash.', 'Sullivan', 'E.O.', 'Rudkoebing', 'Godot', 'Victor', 'Dante', 'Centigrade', 'Amenitskii', 'Attopeu', 'Lucille', 'Philippe', 'Zeffirelli', 'Texan', 'Bellow', 'Leni', 'Karl-Birger', 'Prado', 'OBE', \"D'Artaguette\", 'Simmonsville', 'Schuylkill', 'Interama', 'Marchand', 'Jersey', 'Elliott', 'Centralia', 'Ribes', 'Sargent', 'Abra', 'Theatre-by-the-Sea', 'Braun', 'Tolek', 'Kililngsworth', 'Dodge', 'Congolese', 'Verne', 'Schultz', 'Mencken', 'Lissa', 'B.C.', 'Neck', 'Enver', 'Yuki', 'Bunyan', 'Leona', 'Camelot', 'Veblen', 'Liston', 'Ronnie', 'Miro', 'Nennius', 'Parris', 'Graves', 'Lagoon', 'Phyfe', 'Waldo', 'Sinatra', 'Matunuck', 'Kinsell', 'Wieland', 'Lao', 'Chopin', 'Stevens', 'Lucia', 'Lavallade', 'Faith', 'Summerspace', 'Enfield', 'Worcester', 'Pyrex', 'Fortescue', 'Tobin', 'Messiah', 'Madrid', 'Porto', 'Edwin', 'Flyer-Castle', 'Kyoto', 'Berlin', 'Shoup', 'Fromm', 'Dynafac', 'Fleischmanns', 'Hanover-Bertie', 'Greg', 'Monticello', 'Bertoia', 'Chancellorsville', 'Dunlop', 'Indochina', 'Schmalzried', 'Chase', 'Shakespeare', 'Digby', 'Batavia', 'Karns', 'Judson', 'Haddix', 'Morley', 'Lizzie', 'Kassem', 'Barth', 'Delhi', 'Willard', 'Kempe', 'J', 'Thayer', 'Orwell', 'Schrunk', 'Alice', 'orzae', 'Telefunken', 'OME', 'Ignazio', 'Luke', 'Acala', 'Hempstead', 'Genevieve', 'Sut', 'JNR', 'botulinum', 'Winsor', 'Kekisheva', 'Monilia', 'Stratton', 'Spinrad', 'Mullenax', 'Pemberton', 'Domokous', 'Malcolm', 'Dohnanyi', 'Florentine', 'Koshare', 'Angell', 'Ito', 'Hanover-Chalidale', 'Rosen', 'Saabye', 'Va.', 'Jones', 'Smith-Colmer', 'Worthy', 'Loen', 'Byzantium', 'Poindexter', 'Anders', 'January', 'Pamela', 'Nicodemus', 'Robinson', 'Anderson', 'Bierce', 'Rainy', 'Kiwanis', 'Cantor', 'Lily', 'Cornwallis', 'Farina', 'Gunny', 'DiLuzio', 'Leland', 'Guizot', 'Hanukkah', 'Taraday', 'Fourth-of-July', 'Sun-Times', 'Wilhelm', 'Kentucky', 'Eshleman', 'Windsor', 'Bester', 'Lord', 'Algerian', 'Sturch', 'Baby-dear', 'Heinkel', 'Douglas', 'Regina', 'Beaumont', 'Torquato', 'Remus', 'Arnolphe', 'Swadesh', 'Ansley', 'Danubian', 'Woodrow', 'Allan', 'Pocket', 'Charlottesville', 'Eduard', 'Butterwyn', 'Lowe', 'Ross', 'Brownapopolus', 'Nicholson', 'Doaty', 'Seattle', 'Existentialism', 'Tennyson', 'Hoover', 'Clyfford', 'Perinetti', 'Persia', 'Toch', 'Waters', 'Clarence', 'Gogol', 'Oklahoma!', 'Amy', 'Gonzales', 'Hun', 'Jed', 'Sibling', 'Dandy', 'Bentham', 'Panama', 'Sayers', 'Poseidon', 'Elios', 'Bouvier', 'Thruston', 'Gerstacker', 'Zen', 'Samuels', 'Paxton', 'Viall', 'Uno', 'Nikolais', 'Toulouse-Lautrec', 'Artur', 'Hebrew', 'Mitchell', 'Riviera', 'Hotei', 'Beaverton', 'Rayburn-Johnson', 'Harrison', 'Agoeng', 'Zaroubin', 'Weinstein', 'Managua', 'Brandt', 'Komleva', 'Hitchcock', 'Pualani', 'Whiteleaf', 'Liberal-Radical', 'Shari', 'Chatham', 'Korneyeva', 'Congress', 'Spelman', 'Methodist', 'Reichenberg', 'Fisk', 'Pimen', 'Vikulov', 'Monica', 'Tiepolo', 'Madagascar', 'Littau', 'Korngold', 'Westfield', 'Southampton', 'Pankowski', 'Finland', 'Grigorss', 'Zinman', 'Emanuel', 'Wilbur', 'Bishopsgate', 'Fra', 'Lawrenceville', 'Ellie', 'Suggs', 'Nori', 'Phedre', 'Rorschach', 'Hanover-Lucy', 'Angelico', 'Zhok', 'Grumble', 'Coronado', 'Major', 'Diaghileff', 'Herman', 'V', 'Fahey', 'Harnick', 'Francesco', 'Pascagoula', 'Ford', 'Nathan', 'Adoniram', 'Melies', 'Cunard', 'Elder', 'Sirinjani', 'Nasser', 'TV', 'Eileen', 'Ludwick', 'Ratcliff', 'Shillong', 'Mando', 'Laude', 'Two-Head', 'Alec', 'Boggs', 'Garson', 'Donnybrook', 'Schaeffer', 'Elizabeth', 'Gino', 'Rockfork', 'Orcutt', 'Hopei', 'Nakayasu', 'Miranda', 'Hattie', 'Killpath', 'Storeria', 'Hackstaff', 'Hemus', 'Senora', 'Lehman', 'Flanagan', 'Giovanni', 'Dumpty', 'Bulloch', 'Skiway', 'Curzon', 'Budlong', 'Stephenson', 'Moller', 'Tartarughe', 'Sawyer', 'Cowessett-East', 'Dipylon'], [\"Sihanouk's\", \"Johnnie's\", \"Trafton's\", \"Karipo's\", \"Cleota's\", \"Giffen's\", \"Kowalski's\", \"Heidegger's\", \"Mijbil's\", \"Frenchman's\", \"Lanin's\", \"Anderson's\", \"Veeck's\", \"Davy's\", \"Jim's\", \"Blanchard's\", \"Brannon's\", \"White's\", \"Kirby's\", \"Davis'\", \"Snyder's\", \"Collins'\", \"Amelia's\", \"Kate's\", \"Schopenhauer's\", \"Shartzer's\", \"Rayburn's\", \"Marlowe's\", \"Galtier's\", \"Bosis'\", \"Hudson's\", \"Shann's\", \"Crombie's\", \"Buck's\", \"Painter's\", \"Sorrentine's\", \"California's\", \"Bramante's\", \"Ellison's\", \"Eisenhower's\", \"Lattimer's\", \"Cathy's\", \"Hough's\", \"Curzon's\", \"Philadelphia's\", \"Haydn's\", \"Tom's\", \"Murphy's\", \"Robards'\", \"Red's\", \"Kremlin's\", \"Potemkin's\", \"Lawrence's\", \"Bromfield's\", \"Cleburne's\", \"Griffith's\", \"Hale's\", \"Jaggers'\", \"Carpenter's\", \"Erikson's\", \"Hobbes'\", \"France's\", \"Cambodia's\", \"Erdmann's\", \"Ada's\", \"Patchen's\", \"Warren's\", \"Schubert's\", \"Killpath's\", \"Barton's\", \"Henrietta's\", \"Kemble's\", \"Yorker's\", \"Romeo's\", \"Buzz's\", \"Herman's\", \"Voltaire's\", \"God's\", \"Polk's\", \"Crosby's\", \"Havisham's\", \"Welch's\", \"Jane's\", \"Drexel's\", \"Byrd's\", \"Broglio's\", \"Field's\", \"Leesona's\", \"Kennan's\", \"Rangoni's\", \"Darwin's\", \"Schnabel's\", \"Weinstein's\", \"Pamela's\", \"Jess's\", \"Stephens's\", \"Spencer's\", \"Racine's\", \"Carmer's\", \"Berger's\", \"Providence's\", \"Lowe's\", \"Howe's\", \"Ayres'\", \"Chicago's\", \"Helva's\", \"Arlen's\", \"Jenkins's\", \"Berlin's\", \"Francisco's\", \"Schuman's\", \"Rector's\", \"Jones'\", \"Frost's\", \"Quiney's\", \"Wesley's\", \"Chennault's\", \"Vermont's\", \"Maxim's\", \"Vesuvio's\", \"Magwitch's\", \"Loudon's\", \"Colvin's\", \"Faulkner's\", \"Macaulay's\", \"Pedersen's\", \"Sprague's\", \"VecTrol's\", \"Wheelan's\", \"Beige's\", \"Rose's\", \"Channing's\", \"Lindbergh's\", \"Hubie's\", \"Lublin's\", \"Crouch's\", \"Pantheon's\", \"Abbas's\", \"SAAMI's\", \"Pergolesi's\", \"Wolpe's\", \"Christine's\", \"Ireland's\", \"Fromm's\", \"Lew's\", \"Muck's\", \"House's\", \"Lillian's\", \"Harris'\", \"Munich's\", \"Throneberry's\", \"Cole's\", \"Dickens'\", \"Joan's\", \"Buell's\", \"Sam's\", \"Dufresne's\", \"Harley's\", \"Raphael's\", \"Mays'\", \"Hoijer's\", \"Brooklyn's\", \"Andrei's\", \"Skipjack's\", \"Lindsey's\", \"Lauchli's\", \"Oxford's\", \"Dalton's\", \"Meeker's\", \"Carleton's\", \"Enright's\", \"Joel's\", \"Albright's\", \"John's\", \"Greece's\", \"Blatz's\", \"Craig's\", \"Plato's\", \"Rourke's\", \"Brenner's\", \"Henley's\", \"Seebohm's\", \"Broadway's\", \"Lucas's\", \"Watson's\", \"Cappy's\", \"Stalin's\", \"Sara's\", \"Meyerbeer's\", \"Dwyer's\", \"Hume's\", \"McKinley's\", \"Doaty's\", \"WWRL's\", \"Phouma's\", \"Hogan's\", \"Hillman's\", \"Daniel's\", \"Helion's\", \"Simmons'\", \"Cabot's\", \"Geraghty's\", \"Cibula's\", \"Russell's\", \"Kunkel's\", \"Hugo's\", \"More's\", \"Rankin's\", \"Burnham's\", \"Tillie's\", \"Beardsley's\", \"Donizetti's\", \"Nate's\", \"Offenbach's\", \"Poor's\", \"Griffin's\", \"Flynn's\", \"O'Donnell's\", \"Hughes'\", \"Toffenetti's\", \"Pawtucket's\", \"Stewart's\", \"Dreiser's\", \"Miller's\", \"Tuttle's\", \"Pike's\", \"Klemperer's\", \"Kerr's\", \"Suvorov's\", \"Hood's\", \"Puccini's\", \"Caltech's\", \"Haumd's\", \"Thomas's\", \"Hoge's\", \"Foster's\", \"Fink's\", \"Sidney's\", \"Musil's\", \"Jersey's\", \"Gaylor's\", \"Heidenstam's\", \"Swadesh's\", \"Quasimodo's\", \"Chrysler's\", \"Maxwell's\", \"Dade's\", \"Kayabashi's\", \"Talbott's\", \"Cunard's\", \"Porter's\", \"Nehru's\", \"Gregory's\", \"Art's\", \"Europe's\", \"Montgomery's\", \"Hilprecht's\", \"Selkirk's\", \"Parker's\", \"Georgetown's\", \"Gerosa's\", \"Madonna's\", \"Olson's\", \"Schonberg's\", \"Luis's\", \"Canadian's\", \"Horne's\", \"Antoine's\", \"Bruckner's\", \"Hoyle's\", \"Kornbluth's\", \"Trujillo's\", \"Ritter's\", \"Bootle's\", \"Pockmanster's\", \"Harriet's\", \"Quebec's\", \"Jackson's\", \"Schuyler's\", \"Alcinous'\", \"Wyndham's\", \"Byron's\", \"Greg's\", \"Luther's\", \"Caravan's\", \"Bellamy's\", \"Hesperus'\", \"Nipe's\", \"Diane's\", \"Flotte's\", \"Lewis'\", \"Matsuo's\", \"Gogol's\", \"Lucy's\", \"Thomas'\", \"Ferguson's\", \"Mann's\", \"January's\", \"George's\", \"Eugene's\", \"Lilly's\", \"Meyner's\", \"Mundt's\", \"Holmes'\", \"Jenny's\", \"Anthony's\", \"Leonato's\", \"Baylor's\", \"Cunningham's\", \"Littlepage's\", \"Marsh's\", \"Taney's\", \"Creston's\", \"Saud's\", \"Black's\", \"Hard's\", \"Boniface's\", \"Susan's\", \"Lizzie's\", \"McConnell's\", \"Pullman's\", \"Harrison's\", \"Casey's\", \"Maude's\", \"Ekstrohm's\", \"Sloan's\", \"Aubrey's\", \"Feathertop's\", \"Orwell's\", \"Ball's\", \"Edward's\", \"Wycoff's\", \"Garrard's\", \"Hans'\", \"Caldwell's\", \"Danny's\", \"James's\", \"Greville's\", \"Varlaam's\", \"Marquis'\", \"Winslow's\", \"Charlotte's\", \"Weigel's\", \"Chandler's\", \"Berg's\", \"Laurents'\", \"Georgia's\", \"Lester's\", \"Marr's\", \"Dan's\", \"Houghton's\", \"Kipling's\", \"Tsar's\", \"Stone's\", \"Shea's\", \"London's\", \"Day's\", \"Hemingway's\", \"DeKalb's\", \"Symes's\", \"Adenauer's\", \"Herberet's\", \"Sharpe's\", \"Marcel's\", \"Jesus'\", \"Carla's\", \"Sturley's\", \"Spumoni's\", \"Rome's\", \"Roy's\", \"Pauling's\", \"Lonsdale's\", \"Hamilton's\", \"Massachusetts'\", \"David's\", \"Ptolemy's\", \"Eli's\", \"Wagner's\", \"German's\", \"Seaton's\", \"Wert's\", \"Watson-Watt's\", \"Reavey's\", \"Richardson's\", \"Parry's\", \"Keith's\", \"Eichmann's\", \"McCarthy's\", \"Shelley's\", \"Allison's\", \"MacDonald's\", \"U.S.S.R.'s\", \"Domitian's\", \"Adams's\", \"Pandelli's\", \"Marty's\", \"Remarque's\", \"Augustine's\", \"Larkin's\", \"Moore's\", \"Webster's\", \"Dryfoos'\", \"U.N.'s\", \"Larson's\", \"Franklin's\", \"Cooper's\", \"Loesser's\", \"Churchill's\", \"Letch's\", \"Napoleon's\", \"Maris's\", \"Mickey's\", \"Rousseau's\", \"Wilder's\", \"Burns's\", \"Linda's\", \"Bolingbroke's\", \"Lincoln's\", \"Lowell's\", \"Aristotle's\", \"England's\", \"Harrington's\", \"Anne's\", \"Rowlands'\", \"Fumio's\", \"Spahn's\", \"Pete's\", \"Muller's\", \"Felice's\", \"Segal's\", \"Sukarno's\", \"Yale's\", \"Bultmann's\", \"Jenni's\", \"Furhmann's\", \"Stober's\", \"Howsam's\", \"Segovia's\", \"Carwood's\", \"Deegan's\", \"Hodges'\", \"Miami's\", \"Soviet's\", \"Bridget's\", \"Herry's\", \"Ben's\", \"Button's\", \"Crane's\", \"Mantle's\", \"Pohl's\", \"Goethe's\", \"Jorda's\", \"Menelaus'\", \"Michelangelo's\", \"Globocnik's\", \"Zachrisson's\", \"Dian's\", \"Jefferson's\", \"Brooks's\", \"X's\", \"Whitman's\", \"Manchester's\", \"Morris'\", \"Devey's\", \"Wolfe's\", \"Robinson's\", \"Sandburg's\", \"Gardner's\", \"Brandon's\", \"Manning's\", \"Richard's\", \"Hetty's\", \"Willis'\", \"Oso's\", \"Juanita's\", \"Felix's\", \"Skolman's\", \"Cotten's\", \"Williams'\", \"Barnard's\", \"Lockheed's\", \"Henry's\", \"Nadine's\", \"Lucifer's\", \"Green's\", \"Britain's\", \"Burton's\", \"Atlanta's\", \"Apollo's\", \"Emerson's\", \"Hopkins'\", \"Bright's\", \"Minnesota's\", \"Picasso's\", \"Jellinek's\", \"Burman's\", \"Xydis'\", \"Hegel's\", \"Gannon's\", \"Jennie's\", \"Guthrie's\", \"Rhea's\", \"Poussin's\", \"Salter's\", \"Bonn's\", \"Sheckley's\", \"Lenin's\", \"Sibylla's\", \"Dostoevsky's\", \"Ann's\", \"U.S.'s\", \"Conyers'\", \"Peale's\", \"Harvard's\", \"Curt's\", \"Tech's\", \"Louis's\", \"Gaines'\", \"Tolstoy's\", \"Warsaw's\", \"Barcus'\", \"Pollock's\", \"Sangallo's\", \"Molly's\", \"Brookmeyer's\", \"Mary's\", \"Woodbury's\", \"Congo's\", \"Kruger's\", \"Allen's\", \"Austin's\", \"Finney's\", \"Scott's\", \"Browning's\", \"Kai-shek's\", \"Wilson's\", \"Piepsam's\", \"Johnson's\", \"Colorado's\", \"Kitti's\", \"Constable's\", \"Cuba's\", \"Poitrine's\", \"Wesker's\", \"Kroger's\", \"Rusk's\", \"Doolin's\", \"Thelma's\", \"Wheeler's\", \"Mae's\", \"Mitchell's\", \"Chabrier's\", \"Braque's\", \"America's\", \"Mississippi's\", \"Santayana's\", \"Frelinghuysen's\", \"Custer's\", \"Knill's\", \"Favre's\", \"Alcott's\", \"Krutch's\", \"Pops's\", \"B'dikkat's\", \"Rachel's\", \"Hal's\", \"Round's\", \"Shearing's\", \"Bartoli's\", \"Hirey's\", \"Hausman's\", \"Stavropoulos'\", \"Illinois'\", \"Stengel's\", \"Washizu's\", \"Hrothgar's\", \"Mallory's\", \"Rev's\", \"Istiqlal's\", \"Elec's\", \"Hemenway's\", \"Angeles'\", \"Miranda's\", \"Hoag's\", \"Gosson's\", \"Hammarskjold's\", \"Mark's\", \"Hooker's\", \"Mommy's\", \"Newton's\", \"Poland's\", \"Riverside's\", \"Morgan's\", \"Basho's\", \"Kirov's\", \"Moscow's\", \"Woodcock's\", \"TR.'s\", \"Washington's\", \"Mendelssohn's\", \"Edythe's\", \"Lueger's\", \"Wolff's\", \"Hanover's\", \"Littleton's\", \"Pendleton's\", \"Conrad's\", \"Jesus's\", \"Kelley's\", \"Wheelock's\", \"Grey's\", \"Floyd's\", \"Sparling's\", \"Gordon's\", \"Meynell's\", \"Blish's\", \"Victor's\", \"Hammett's\", \"Jap's\", \"O'Connor's\", \"Parkinson's\", \"Capone's\", \"Hindemith's\", \"Basil's\", \"Stanley's\", \"Best's\", \"Shirley's\", \"Hansen's\", \"Bobbie's\", \"Gargery's\", \"Lagrange's\", \"Barco's\", \"Feyer's\", \"Lewis's\", \"Rexroth's\", \"Frayne's\", \"Hall's\", \"Spinley's\", \"Nischwitz'\", \"Erasmus's\", \"Moniuszko's\", \"Boston's\", \"Ruth's\", \"Levinger's\", \"Milton's\", \"McPherson's\", \"Coward's\", \"Kahn's\", \"Thom's\", \"Shayne's\", \"Jessica's\", \"Doris'\", \"Mulligan's\", \"Gracie's\", \"Connolly's\", \"Vonnegut's\", \"Dowling's\", \"Hollywood's\", \"Richards'\", \"Power's\", \"Simonson's\", \"Emmett's\", \"Florida's\", \"Journal-Bulletin's\", \"Thurber's\", \"Al's\", \"Mozart's\", \"O'Banion's\", \"Monmouth's\", \"Tilghman's\", \"Vienna's\", \"Burch's\", \"Mathias'\", \"Sarah's\", \"Shafer's\", \"Frankfurter's\", \"Rod's\", \"Senate's\", \"Hendricks'\", \"Catherine's\", \"Shaefer's\", \"Abel's\", \"Karns'\", \"Hardee's\", \"Beebe's\", \"Runyon's\", \"Vance's\", \"Merton's\", \"Anniston's\", \"Beckett's\", \"Miriani's\", \"Pocket's\", \"Walter's\", \"Macklin's\", \"Garibaldi's\", \"Stout's\", \"Stowe's\", \"Stein's\", \"Sweden's\", \"Phil's\", \"Palfrey's\", \"Pagnol's\", \"China's\", \"Cargill's\", \"Fritzie's\", \"Volta's\", \"Yoshimoto's\", \"Dresbach's\", \"Ruark's\", \"Arnold's\", \"Newport's\", \"Wright's\", \"Beowulf's\", \"Jerry's\", \"Robby's\", \"Ruger's\", \"Missouri's\", \"Louis'\", \"Arkansas'\", \"Hitler's\", \"Keats's\", \"Fiedler's\", \"Schiele's\", \"Dandy's\", \"Wells's\", \"Harburg's\", \"Wally's\", \"Perrin's\", \"Nagrin's\", \"Harmony's\", \"Dean's\", \"Baltimore's\", \"Cervantes'\", \"Skyros'\", \"Balaguer's\", \"Uno's\", \"Mercer's\", \"Groth's\", \"Eileen's\", \"Lenygon's\", \"Dexter's\", \"Roylott's\", \"Whipple's\", \"Alusik's\", \"Moliere's\", \"Clayton's\", \"Negro's\", \"Woolworth's\", \"Snow's\", \"Borromini's\", \"Canada's\", \"Bradbury's\", \"Horace's\", \"Lee's\", \"Othon's\", \"Brumidi's\", \"Lalaurie's\", \"Carreon's\", \"Israel's\", \"Granny's\", \"Tower's\", \"Katherine's\", \"Hagerty's\", \"Bill's\", \"Verdi's\", \"Squire's\", \"Dick's\", \"Huxley's\", \"Meltzer's\", \"Toscanini's\", \"Lopatnikoff's\", \"Fistoulari's\", \"Charles'\", \"Pilate's\", \"N.C.'s\", \"Henri's\", \"Riemann's\", \"Coolidge's\", \"Nugent's\", \"Montero's\", \"Arbuckle's\", \"Carl's\", \"Trevelyan's\", \"Orlick's\", \"Moriarty's\", \"Colcord's\", \"Fred's\", \"Faget's\", \"Colt's\", \"Milwaukee's\", \"Pettigrew's\", \"Gannett's\", \"Admassy's\", \"Haupts'\", \"Ontario's\", \"Sophia's\", \"Garth's\", \"Newbiggin's\", \"Chavez'\", \"Askington's\", \"Sande's\", \"B.'s\", \"Slocum's\", \"Marx's\", \"Joe's\", \"Nixon's\", \"Philip's\", \"Mills's\", \"William's\", \"Hino's\", \"Reily's\", \"Brassbound's\", \"Payne's\", \"Berto's\", \"Schumann's\", \"Tussard's\", \"Oldenburg's\", \"Prokofieff's\", \"Thor's\", \"Sally's\", \"Lyford's\", \"Koussevitzky's\", \"Scotty's\", \"Faust's\", \"Manhattan's\", \"Shaw's\", \"Indian's\", \"Beethoven's\", \"Brush-off's\", \"Nicolas's\", \"Betty's\", \"Katie's\", \"Benson's\", \"Mirsky's\", \"Bizet's\", \"Virginia's\", \"Darling's\", \"Oliver's\", \"Doyle's\", \"Dunn's\", \"Leavitt's\", \"Underwood's\", \"Hyde's\", \"Carolina's\", \"Nick's\", \"McCloy's\", \"Copernicus'\", \"Milhaud's\", \"Powell's\", \"Hamrick's\", \"Ramey's\", \"Miriam's\", \"Ward's\", \"Graves'\", \"Stratford's\", \"Woodruff's\", \"Todman's\", \"Fran's\", \"Crosson's\", \"Housman's\", \"Alexander's\", \"Paula's\", \"Child's\", \"Pam's\", \"Fogg's\", \"Stevenson's\", \"Young's\", \"Cicero's\", \"Mahler's\", \"Land's\", \"Gee's\", \"Sherman's\", \"Blevins'\", \"Titche's\", \"Xavier's\", \"Germany's\", \"Hope's\", \"Willie's\", \"Dronk's\", \"Irenaeus'\", \"Caton's\", \"Jubal's\", \"Wisman's\", \"February's\", \"Alfred's\", \"Maryland's\", \"Mongolia's\", \"Diman's\", \"Francie's\", \"Detroit's\", \"Tim's\", \"Tamiris'\", \"Chopin's\", \"McClellan's\", \"Clark's\", \"Coleridge's\", \"Kramer's\", \"Harris's\", \"Castro's\", \"Needham's\", \"Medfield's\", \"Auerbach's\", \"Cimabue's\", \"Gun's\", \"Tennessee's\", \"Walsh's\", \"Formby's\", \"Portland's\", \"Woods's\", \"Cromwell's\", \"Gladden's\", \"Millay's\", \"Dana's\", \"Maxine's\", \"Morse's\", \"Case's\", \"Perier's\", \"Delphine's\", \"Rossilini's\", \"Gore's\", \"En-lai's\", \"Geely's\", \"ROK's\", \"Madden's\", \"Tucker's\", \"Haughton's\", \"Huff's\", \"Bernini's\", \"Spillane's\", \"Angelo's\", \"Denny's\", 'Grevyles', \"Podger's\", \"Hawkins'\", \"Italy's\", \"Krim's\", \"Injun's\", \"Jannequin's\", \"Pimen's\", \"Goulding's\", \"Lumumba's\", \"Hetman's\", \"Acheson's\", \"Lolotte's\", \"Locke's\", \"Seward's\", \"Ingleside's\", \"Mike's\", \"Grigori's\", \"Fraud's\", \"Augusta's\", \"Pasadena's\", \"Smith's\", \"Edison's\", \"Nasser's\", \"Taylor's\", \"Claudia's\", \"Harlem's\", \"Durrell's\", \"Fleisher's\", \"Luke's\", \"Pip's\", \"Coughlin's\", \"Gavin's\", \"Roberts'\", \"Fokine's\", \"Comedie's\", \"Lord's\", \"Ortega's\", \"Slater's\", \"Rheinholdt's\", \"Leeds'\", \"Rembrandt's\", \"Congress'\", \"Cezanne's\", \"Twain's\", \"Thompson's\", \"Palmer's\", \"M.E.'s\", \"Ahmad's\", \"Whitehead's\", \"Jacoby's\", \"Dickey's\", \"Banks's\", \"Fosdick's\", \"James'\", \"Dave's\", \"Breckenridge's\", \"Bang-Jensen's\", \"Hock's\", \"Mossberg's\", \"Marlin's\", \"Jack's\", \"Gabriel's\", \"Blanche's\", \"Partlow's\", \"Freeman's\", \"M.'s\", \"Faber's\", \"Jed's\"], ['regulating', 'risking', 'naming', 'circling', 'courting', 'stupefying', 'broadcasting', 'drinking', 'Receiving', 'transforming', 'qualifying', 'juggling', 'shimming', 'arranging', 'resisting', 'binding', 'fattening', 'accommodating', 'chopping', 'examining', 'pinging', 'reprobating', 'camping', 'paraphrasing', 'affirming', 'denoting', 'placating', 'imposing', \"Followin'\", 'moving', 'Referring', 'manning', 'regarding', \"killin'\", 'disdaining', 'lingering', 'a-raising', 'knocking', 'descending', 'Fuming', 'glowing', 'appealing', 'twittering', 'hearing', \"callin'\", 'breeding', 'Knowing', 'feeling', 'prospering', 'brooding', 'programming', 'pilfering', 'swelling', 'Begging', 'Washing', 'Despising', 'echoing', 'declaring', 'caressing', 'selling', 'sullying', 'roaming', 'notifying', 'awarding', 'festering', 'dismounting', 'forcing', 'bellowing', 'Pausing', 'dispersing', 'nearing', 'towering', 'teaming', 'popping', 'Shouldering', 'awakening', \"referrin'\", 'stomping', 'sanding', 'Cursing', 'thrusting', 'reserving', 'snowing', 'dividing', 'bursting', 'finishing', 'oscillating', 'Aging', 'sweating', 'indicating', 'distinguishing', 'sluicing', 'murdering', 'consorting', 'accumulating', 'reviving', 'interlacing', 'Sprouting', 'rinsing', 'scrubbing', 'remaining', 'remanding', 'shifting', 'Drifting', 'looming', 'prolonging', 'Moving', 'tiptoeing', 'liquidating', 'kidding', 'dumping', 'Crossing', 'insisting', 'Springing', 'hurling', 'distributing', 'using', 'thickening', \"askin'\", 'spelling', 'harboring', 'bottling', 'Neglecting', 'incurring', 'seceding', 'hindering', 'transporting', 'committing', 'pretending', 'purporting', 'thrumming', 'conferring', 'braiding', 'Sighing', 'portraying', 'Itching', 'flapping', 'creaking', 'Multiplying', 'yielding', 'Maintaining', 'ginning', 'gagging', 'residing', 'marring', 'screeching', 'providing', 'dueling', 'Ionizing', 'trusting', 'contouring', 'feasting', 'varying', 'weaning', 'haranguing', 'exchanging', 'bickering', 'jangling', 'parading', 'masterminding', 'pausing', 'linking', 'hiring', 'consolidating', 'recounting', 'recurring', 'itching', 'resurrecting', 'entering', 'abounding', 'nursing', 'Walking', 'outscoring', 'structuring', 'concentrating', 'searing', 'flashing', 'governing', 'writing', 'Wishing', 'grunting', 'flicking', 'fronting', 'pervading', 'struggling', 'offending', 'clearing', 'perishing', 'incorporating', 'phrasing', 'Matching', 'indexing', 'wakening', 'owing', 'exerting', 'Rising', 'Touring', 'tormenting', 'milling', 'intruding', 'representing', 'choosing', 'Handing', 'bumping', 'suffering', 'partaking', 'Increasing', 'bleating', 'Spraying', 'disheartening', 'ironing', 'interviewing', 'lettering', 'slanting', 'Heading', 'subduing', 'centering', 'pinpointing', 'judging', 'justifying', 'transcending', 'Underlying', 'sneering', 'whizzing', 'rolling', 'issuing', 'marking', \"gettin'\", 'exploring', 'drifting', 'ballooning', 'Finding', 'conserving', 'reducing', 'resting', 'lisping', 'sightseeing', 'beaming', 'enameling', 'emphasizing', 'pestering', 'decanting', 'surrendering', 'fawning', 'realizing', 'cowering', 'bending', 'brandishing', 'Toasting', 'play-acting', 'combing', 'perspiring', 'addressing', \"goin'\", 'imbruing', 'Slipping', 'separating', 'scrambling', 'financing', 'following', 'checking', 'commencing', 'interpreting', 'reminding', 'deriving', 'believing', 'crawling', 'whisking', 'disintegrating', 'receiving', 'Fitting', 'interlocking', 'vilifying', 'smoothing', 'jetting', 'proceeding', 'shadowing', 'boating', 'thanking', 'aching', 'Soaring', 'furthering', \"throwin'\", 'unleashing', 'ringing', 'moderating', 'mitigating', 'snoring', 'Thinking', 'blanching', 'gleaming', 'tensioning', 'pioneering', 'pricing', 'restating', 'hunting', 'blotting', 'shading', 'squealing', 'conducting', 'blushing', 'overhauling', 'zooming', 'Retiring', 'bewitching', 'restraining', 'ceasing', 'fouling', 'clapping', 'yodeling', 'grasping', 'desiring', 'identifying', 'resuming', 'fanning', 'trembling', 'driving', 'corroding', 'settling', 'waterskiing', 'blaming', 'forgiving', 'running', 'aiding', 'registering', 'clucking', 'starring', 'speaking', 'certifying', 'coiling', 'tightening', 'hiding', 'sipping', 'Looking', 'Gaining', 'a-crowing', 'Observing', 'softening', 'engineering', 'dancing', 'smooching', 'waggling', 'Grabbing', \"redeemin'\", 'travelling', 'filling', 'foregoing', 'quivering', 'doting', 'totaling', 'flourishing', 'plowing', 'Tampering', 'Selecting', 'cherishing', 'patting', 'mulching', 'ragging', 'presuming', 'crossing', 'hogging', 'wallowing', 'gasping', 'gnawing', 'hurting', 'marshaling', 'bolting', 'jollying', 'resembling', 'thieving', 'associating', 'storming', 'wandering', 'priming', 'bemaddening', 'enticing', 'pinning', 'drawing', 'budgeting', 'amending', 'Putting', 'filtering', 'licensing', 'nagging', 'drawling', 'choking', 'growling', 'solvating', 'straddling', 'contaminating', 'chipping', 'redeeming', 'researching', 'printing', 'rediscovering', 'funding', 'troubling', 'Engineering', 'adopting', 'lurking', 'relaxing', 'watching', 'distracting', 'decentralizing', 'spitting', 'waiting', 'Popping', 'stealing', 'eyeing', 'recommending', 'promoting', 'fearing', 'saving', 'By-passing', 'attempting', 'overbearing', 'bounding', 'engaging', 'iodinating', \"meetin'\", \"sportin'\", 'Nourishing', 'mating', \"Rammin'\", 'slowing', 'signaling', 'coordinating', 'interweaving', 'shaving', 'fielding', 'Thermoforming', 'lining', 'reiterating', 'piping', 'teetering', 'expressing', 'beckoning', 'bluffing', 'steaming', 'lyking', 'kicking', 'wincing', 'shoving', 'involving', \"bustlin'\", 'coupling', 'remodeling', 'meandering', 'referring', 'Lighting', 'splashing', 'Leaning', 'breaking', 'harvesting', 'proving', 'retailing', 'pooling', 'capturing', 'unclasping', 'brushing', 'transplanting', 'glowering', 'circumscribing', 'luring', 'foaming', 'helping', 'relying', 'remounting', 'ruining', 'reacting', 'singling', 'folding', 'puckering', 'Cutting', 'jeopardizing', 'hanging', 'availing', 'strutting', 'fulminating', 'woodworking', 'publicizing', 'enlarging', 'detonating', 'Rapping', 'quaking', 'coagulating', 'rhyming', 'leaping', 'cancelling', 'divining', 'Harnessing', 'averaging', 'continuing', 'wheeling', 'peeling', 'generating', 'napping', 'catching', 'mortaring', 'straightening', 'mocking', 'regaining', 'proposing', \"Shootin'\", 'bristling', 'resulting', 'culminating', 'sighing', 'grinning', 'purpling', 'rigging', 'landscaping', 'gesticulating', 'comforting', 'practising', 'rummaging', 'hoping', 'securing', 'inflicting', 'enforcing', 'stranding', 'controlling', 'dozing', 'exporting', 'showering', 'flushing', 'designating', 'flowering', 'fighting', 'smoking', 're-establishing', 'sweltering', 'puttering', 'implementing', 'exclaiming', 'fencing', \"wearin'\", 'typifying', 'leading', 'mailing', 'hurrying', 'calling', 'teasing', 'Marketing', 'shocking', 'sunning', 'idling', 'pelting', 'economizing', 'pursuing', 'debunking', 'creeping', 'scathing', 'evaluating', 'evoking', 'breathing', 'authorizing', 'disrupting', 'divulging', 'spying', 'rounding', 'despairing', 'directing', 'Recounting', 'applying', 'motioning', 'halting', 'proselytizing', 'appraising', 'warbling', 'whimpering', 'cheering', 'minifying', 'pulsating', 'purchasing', 'coursing', 'feigning', \"travelin'\", 'testing', 'Seeming', 'admitting', 'dining', 'sailing', 'Joking', 'unlocking', 'muttering', 'comparing', 'Interviewing', 'marbleizing', 'reconditioning', 'considering', 'filing', 'Keeping', 'Caring', \"showin'\", 'attaining', 'healing', 'farming', 'plunking', 'slashing', 'tampering', 'bowing', 'specializing', 'prodding', \"wrappin'\", 'conversing', 'balancing', 'handling', 'sneaking', 'pounding', 'editing', 'threading', 'Complying', \"herdin'\", 'fleeing', 'conveying', 'communicating', 'messing', 'opposing', 'wanting', 'sneezing', 'acquiring', 'maturing', 'allowing', 'coning', 'buckling', 'shuttling', 'grazing', 'starving', 'Borrowing', 'empowering', 'leveling', 'discussing', 'philosophizing', 'entertaining', 'haggling', 'foreclosing', 'Citing', 'enacting', 'integrating', 'dishonouring', 'Shivering', 'beating', 'preventing', 'frothing', 'starting', 'sampling', 'sucking', \"gallivantin'\", 'bawling', 'satisfying', 'itemizing', 'delving', 'defocusing', 'bouncing', 'purling', 'celebrating', 'pointing', 'kidnapping', 'multiplying', 'promulgating', 'defending', 'dramatizing', \"a-stoopin'\", \"rubbin'\", 'intersecting', 'Visiting', 'bailing', \"Shippin'\", 'stabilizing', 'crimsoning', 'caring', 'clanking', 'scrutinizing', 'retiring', \"peggin'\", 'bathing', 'crowning', 'short-changing', 'advising', 'renewing', 'persuading', 'Returning', 'striding', 'boosting', 'wounding', 'muddling', 'waning', 'Rummaging', 'preening', 'functioning', 'Eating', 'manifesting', 'dwindling', 'Arriving', 'deeming', 'Teaching', 'sockdologizing', 'nudging', 'swaying', 'Sipping', 'Separating', 'Starting', 'prosecuting', 'slacking', 'whistling', 'missing', 'conceding', \"brandin'\", 'necking', 'kissing', 'lengthening', 'whipping', 'lapsing', 'lacking', 'Wondering', 'borrowing', 'impaling', 'beautifying', 'comprehending', 'snarling', 'blinking', 'denying', 'Assuming', 'Engaging', 'scouring', 'preponderating', 'numbing', 'layering', 'dictating', 'riddling', 'praying', 'supplanting', 'changing', 'exiling', 'construing', 'balking', 'Hauling', 'nationalizing', \"Countin'\", 'waitin', 'crumbling', 'vending', 'appropriating', 'Hiring', 'petting', 'squashing', 'donning', 'upsetting', 'measuring', 'passing', 'discharging', 'decrying', 'Riding', 'bubbling', 'figuring', 'rehabilitating', 'gnashing', 'gossiping', 'disking', 'sentencing', 'burning', 'demanding', 'motivating', 'footing', 'seeming', 'strumming', 'exploiting', 'prying', 'flinching', 'trying', 'zoning', 'polarizing', 'adding', 'gazing', 'hitching', 'dealing', 'explaining', 'stepping', 'Extending', 'raping', 'coloring', 'riding', 'grading', 'heating', 'contenting', 'renouncing', 'encouraging', \"standin'\", 'impending', 'curing', 'crossroading', 'Tossing', 'carousing', 'constricting', 'affording', 'lapping', 'erecting', 'tantalizing', 'behaving', 'obeying', 'scanning', 'carping', 'orienting', 'indwelling', 'Swinging', 'meditating', 'diluting', 'poring', 'Gathering', 'extruding', 'crusading', 'radiating', 'curling', 'throbbing', 'enhancing', 'Sporting', 'cooling', 'tasting', 'blockading', 'arguing', 'clasping', \"grazin'\", 'Leaving', 'preceding', 'showing', 'noting', 'conning', 'Owing', 'entrusting', 'tinkering', 'mouthing', 'sacking', 'Financing', 'scheming', 'reliving', 'hemorrhaging', 'foreseeing', 'foreboding', 'Displaying', 'tugging', 'two-timing', 'alarming', 'rehearsing', 'leasing', 'traveling', 'racking', 'heaving', 'expounding', 'attributing', 'disseminating', 'assaulting', 'overturning', 'retaining', 'cleansing', 'skiing', 'scolding', 'Burning', 'assisting', 'professing', 'allotting', 'jolting', 'Seeing', 'veiling', 'languishing', 'serving', 'squirting', 'belching', 'reorganizing', 'strolling', 'incriminating', 'arousing', 'countervailing', 'undulating', 'befuddling', 'existing', \"chousin'\", 'littering', \"buildin'\", 'contending', 'disabling', 'trimming', 'greying', 'computing', 'subtracting', 'stooping', 'concealing', 'sharing', 'peeping', 'dreaming', 'sitting', 'Preserving', 'soldiering', 'interposing', 'discounting', 'slaying', 'waving', 'toppling', 'smelling', 'recording', 'raging', 'charting', 'crying', 'taking', 'spurring', 'overeating', 'awaiting', 'pulverizing', 'stretching', 'spraying', 'bolstering', 'expediting', 'fitting', 'stropping', 'intensifying', 'curving', 'canceling', 'Traveling', 'angling', 'clarifying', 'barging', 'standing', 'fashioning', 'slopping', 'besetting', 'thriving', 'exhibiting', 'growing', 'sickening', 'clustering', 'sparing', 'lecturing', 'swinging', 'domineering', 'rewriting', 'gouging', 'jockeying', 'dotting', 'differing', 'accompanying', 'tinkling', 'marinating', 'compelling', 'according', 'Strolling', 'interacting', 'turning', 'potting', 'reasoning', 'persisting', 'swamping', 'possessing', 'Getting', 'enclosing', 'Dangling', 'navigating', 'raiding', 'volunteering', 'mistaking', 'hauling', 'Failing', 'lending', 'Assisting', 'reversing', 'Standing', 'neighboring', \"ginnin'\", 'basing', 'Hitting', 'indulging', 'Tracing', 'Bathing', 'setting', 'drying', 'devastating', 'consisting', 'Going', 'furnishing', 'refusing', 'hastening', 'restoring', 'diverting', 'rousing', 'dominating', 'Bursting', 'planning', 'enrolling', 'smothering', 'crucifying', 'Bleaching', 'Calling', 'stultifying', 'producing', 'welcoming', 'enveloping', 'burrowing', 'Campaigning', 'drooping', 'mingling', 'imploring'], ['sizzled', 'expelled', 'cracked', 'consented', 'warmed', 'doubted', 'ignored', 'flared', 'waxed', 'insinuated', 'prospered', 'lanced', 'reviewed', 'climaxed', 'recollected', 'rustled', 'steadied', 'imparted', 'hankered', 'mouthed', 'clomped', 'attracted', 'confided', 'rummaged', 'registered', 'referred', 'supported', 'broadened', 'accounted', 'eroded', 'coasted', 'traced', 'slipped', 'perspired', 'congregated', 'went', 'bent', 'heard', 'giggled', 'supplemented', 'leapt', 'blasted', 'chuckled', 'loosened', 'averaged', 'zipped', 'organized', 'affirmed', 'furrowed', 'dissipated', 'revved', 'clashed', 'motivated', 'employed', 'hurt', 'required', 'pertained', 'remanded', 'disguised', 'clenched', 'trailed', 'preceded', 'bound', 'constituted', 'smelt', 'hesitated', 'clamored', 'pointed', 'reverted', 'partook', 'astonished', 'leveled', 'spotted', 'thout', 'hatched', 'provoked', 'redefined', 'skirmished', 'favored', 'regulated', 'commanded', 'mismanaged', 'incited', 'chorused', 'co-operated', 'lapped', 'dropped', 'waited', 'voted', 'rescued', 'outraged', 'puzzled', 'coupled', 'concerned', 'Scared', 'nullified', 'sought', 'pushed', 'lavished', 'converted', 'detonated', 'commuted', 'fielded', 'skidded', 'splashed', 'unhitched', 'sapped', 'cooked', 'ascended', 'compressed', 'prophesied', 'confessed', 'parboiled', 'reminisced', 'adjourned', 'messed', 'scrawled', 'reported', 'collected', 'imitated', 'entrenched', 'reinforced', 'unfolded', 'discounted', 'lathered', 'munched', 'lashed', 'thumbed', 'dived', 'capitulated', 'enlisted', 'slowed', 'lingered', 'shied', 'mourned', 'sympathized', 'whacked', 'qualified', 'inquired', 'included', 'tired', 'flooded', 'stared', 'yielded', 'dined', 'needed', 'abandoned', 'revamped', 'petitioned', 'volunteered', 'penetrated', 'snickered', 'pinched', 'zoomed', 'disagreed', 'muffled', 'owed', 'forgot', 'sustained', 'ordered', 'exhaled', 'wheeled', 'overreached', 'merged', 'threatened', 'imbedded', 'parted', 'reeked', 'whispered', 'resulted', 'rammed', 'brushed', 'stretched', 'loused', 'contended', 'mistook', 'lessened', 'comprised', 'poised', 'weighted', 'liked', 'admonished', 'suppressed', 'radioed', 'heisted', 'substituted', 'proposed', 'cost', 'grimaced', 'rimmed', 'disturbed', 'scuttled', 'precluded', 'babbled', 'saw', 'showered', 'revolutionized', 'revolted', 'frowned', 'placed', 'influenced', 'characterized', 'visited', 'dreaded', 'decreed', 'dented', 'peered', 'twined', 'pummeled', 'glanced', 'retraced', 'filtered', 'barred', 'half-reached', 'motioned', 'hunted', 'mentioned', 'forgave', 'wrenched', 'installed', 'stomped', 'stepped', 'slugged', 'wrote', 'gasped', 'led', 'altered', 'returned', 'twisted', 'experienced', 'produced', 'spelled', 'wove', 'underwent', 'mocked', 'overran', 'stammered', 'willed', 'bragged', 'remarried', 'delivered', 'peeked', 'pfffted', 'transgressed', 'cocked', 'pantomimed', 'nettled', 'applauded', 'lounged', 'paired', 'plopped', 'panted', 'forced', 'reeled', 'engaged', 'revolved', 'gobbled', 'bothered', 'vibrated', 'blurted', 'deserted', 'crouched', 'interested', 'recommended', 'involved', 'locked', 'meant', 'postponed', 'marvelled', 'cropped', 'guided', 'felt', 'flopped', 'heralded', 'rendered', 'plugged', 'miscalculated', 'prayed', 'infuriated', 'cited', 'espoused', 'unscrewed', 'illuminated', 'bored', 'doomed', 'gagged', 'stiffened', 'berated', 'built', 'tightened', 'styled', 'wired', 'checked', 'evaded', 'dug', 'erred', 'harshened', 'responded', 'urged', 'jarred', 'illumined', 'stunk', 'hugged', 'blossomed', 'tipped', 'quacked', 'presented', 'happened', 'elicited', 'awaited', 'dragged', 'applied', 'rumbled', 'proved', 'buckled', 'adapted', 'shrilled', 'stumbled', 'paralleled', 'swooped', 'plied', 'risked', 'wadded', 'rearranged', 'deemed', 'introduced', 'validated', 'uncorked', 'clipped', 'endorsed', 'loomed', 'glistened', 'held', 'dammed', 'timed', 'found', 'originated', 'condemned', 'unwired', 'overtook', 'swirled', 'gouged', 'fluttered', 'advertised', 'reacted', 'nominated', 'snuck', 'consulted', 'halted', 'totaled', 'glued', 'flipped', 'Exclaimed', 'proliferated', 'crackled', 'convinced', 'discovered', 'trafficked', 'prohibited', 'rubbed', 'transpired', 'attested', 'mastered', 'boggled', 'rippled', 'slurped', 'tabulated', 'wondered', 'eyed', 'enabled', 'construed', 'haunted', 'towed', 'detached', 'pioneered', 'skimmed', 'imposed', 'kneeled', 'overrode', 'dwelt', 'moored', 'flickered', 'nested', 'flourished', 'mingled', 'wangled', 'etched', 'exchanged', 'shot', 'spouted', 'nuzzled', 'commenced', 'said', 'plagued', 'promoted', 'rivaled', 'yearned', 'noted', 'misled', 'belted', 'pitched', 'caused', 'utilized', 'bumped', 'deceived', 'drewe', 'bulked', 'viewed', 'fell', 'predominated', 'chattered', 'interfered', 'spiraled', 'taxed', 'mooed', 'chanced', 'marveled', 'flapped', 'thought', 'conferred', 'worshipped', 'overheard', 'lasted', 'stalled', 'swarmed', 'commended', 'littered', 'coiled', 'spoiled', 'distinguished', 'charmed', 'confined', 'eased', 'typed', 'handled', 'obliged', 'ascribed', 'slapped', 'expended', 'Sat', 'trained', 'centered', 'conspired', 'sang', 'wrought', 'loped', 'inspected', 'passed', 'incurred', 'undertook', 'popped', 'incanted', 'subtracted', 'protruded', 'shattered', 'guaranteed', 'implored', 'coincided', 'belied', 'lucked', 'affied', 'thundered', 'reined', 'crooned', 'divided', 'aggravated', 'discarded', 'crunched', 'undressed', 'smashed', 'collaborated', 'reserved', 'hauled', 'lugged', 'rebuffed', 'renewed', 'relinquished', 'indicated', 'lightened', 'snorted', 'broke', 'manufactured', 'speeded', 'diagnosed', 'replied', 'lectured', 'scooted', 'faded', 'whizzed', 'scoffed', 'refused', 'answered', 'trilled', 'pursed', 'ground', 'let', 'crept', 'glided', 'boiled', 're-emerged', 'scrubbed', 'patented', 'redoubled', 'delayed', 'ventured', 'signaled', 'hammered', 'detained', 'struggled', 'sashayed', 'bestowed', 'gripped', \"bulletin'd\", 'unlashed', 'rushed', 'dealt', 'tumbled', 'won', 'stamped', 'ministered', 'pouted', 'examined', 'streaked', 'neglected', 'outmatched', 'glommed', 'coaxed', 'nudged', 'throbbed', 'rested', 'stuck', 'sparked', 'scorched', 'roped', 'yelled', 'maligned', 'screeched', 'lacked', 'disliked', 'spit', 'launched', 'side-stepped', 'emerged', 'squealed', 'divorced', 'distorted', 'ticked', 'devoted', 'interviewed', 'sulked', 'blighted', 'celebrated', 'reddened', 'tracked', 'fumbled', 'scheduled', 'discontinued', 'paused', 'underscored', 'talked', 'catapulted', 'demanded', 'chopped', 'provided', 'embarrassed', 'set', 'unfastened', 'pleased', 'flowed', 'smelled', 'cheered', 'spurred', 'scratched', 'dictated', 'opted', 'whined', 'figured', 'worsened', 'batted', 'thrived', 'revived', 'switched', 'whitened', 'fortified', 'intoned', 'recurred', 'discouraged', 'preferred', 'fanned', 'harried', 'implied', 'encircled', 'memorized', 'Came', 'replaced', 'ransacked', 'ripened', 'ran', 'headed', 'overcame', 'tripled', 'pressed', 'expected', 'wore', 'tended', 'folded', 'credited', 'staggered', 'counted', 'skirted', 'dialed', 'embraced', 'disobeyed', 'entertained', 'liberated', 'matched', 'cashed', 'lumbered', 'imbibed', 'erupted', 'poisoned', 'rejoiced', 'arched', 'approached', 'blustered', 'operated', 'wiggled', 'absorbed', 'called', 'wrapped', 'sheered', 'wandered', 'Asked', 'scraped', 'contributed', 'poured', 'catalogued', 'parlayed', 'draped', 'clinched', 'disbelieved', 'blazed', 'patronized', 'expired', 'demonstrated', 'belched', 'feared', 'retreated', 'unveiled', 'wisecracked', 'arrived', 'foamed', 'opened', 'soared', 'quoted', 'printed', 'refolded', 'darkened', 'resumed', 'pattered', 'bit', 'satisfied', 'blew', 'Became', 'covered', 'hummed', 'beat', 'ruffled', 'moaned', 'cultivated', 'irritated', 'plumbed', 'guessed', 'slopped', 'Ran', 'presaged', 'sickened', 'burrowed', 'ghosted', 'prefaced', 'summed', 'hunched', 'baked', 'curtseyed', 'flanked', 'bewildered', 'tripped', 'sharpened', 'boomed', 'softened', 'failed', 'dozed', 'centralized', 'flamed', 'catered', 'travelled', 'straddled', 'cherished', 'awoke', 'knuckled', 'swayed', 'Replied', 'unleashed', 'removed', 'stored', 'grokked', 'exceeded', 'twigged', 'vindicated', 'translated', 'clambered', 'fumed', 'scowled', 'possessed', 'grumbled', 'creaked', 'besmirched', 'boasted', 'charged', 'cackled', 'misjudged', 'submitted', 'slashed', 'read', 'rotated', 'sprinted', 'ranked', 'delighted', 'run', 'painted', 'belonged', 'swished', 'listed', 'traversed', 'worried', 'putted', 'nationalized', 'dispelled', 'suggested', 'admitted', 'tramped', 'drank', 'convicted', 'invited', 'narrowed', 'intensified', 'confounded', 'flounced', 'succeeded', 'panicked', 'aspired', 'declaimed', 'restored', 'flaunted', 'brok', 'decorated', 'informed', 'symbolized', 'tapped', 'ricocheted', 'knelt', 'Stroked', 'explained', 'edified', 'walloped', 'flew', 'made', 'generated', 'exuded', 'shed', 'poked', 'sparkled', 'honeymooned', 'banished', 'accepted', 'tagged', 'awakened', 'abated', 'destroyed', 'crashed', 'amused', 'attached', 'followed', 'telephoned', 'disposed', 'organised', 'inherited', 'fused', 'glared', 'formed', 'tootley-toot-tootled', 'stated', 'lapsed', 'sifted', 'rattled', 'clamped', 'undulated', 'bellowed', 'drummed', 'beckoned', 'interjected', 'convened', 'cleaned', 'prescribed', 'encased', 'tugged', 'mined', 'over-corrected', 'parked', 'plunged', 'padlocked', 'averted', 'druncke', 'greeted', 'sobbed', 'associated', 'deepened', 'beset', 'granted', 'subsided', 'harassed', 'netted', 'necessitated', 'assaulted', 'drowned', 'picked', 'peeled', 'competed', 'buried', 'Thought', 'challenged', 'Startled', 'clutched', 'induced', 'estimated', 'joined', 'investigated', 'deserved', 'watched', 'remained', 'streamed', 'rocked', 'socked', 'advised', 'enraged', 'raked', 'treated', 'journeyed', 'attributed', 'sensed', 'massacred', 'robbed', 'bayed', 'rose', 'stuffed', 'shrieked', 'experimented', 'assembled', 'puffed', 'lunged', 'spared', 'faltered', 'clicked', 'deployed', 'bolted', 'marooned', 'bugged', 'cursed', 'intended', 'warranted', 'sweated', 'plotted', 'pooched', 'requisitioned', 'hooted', 'expressed', 'recounted', 'circulated', 'composed', 'allocated', 'liquidated', 'gauged', 'crumpled', 'roused', 'relaxed', 'gained', 'eloped', 'dipped', 'regained', 'manifested', 'fed', 'plowed', 'initiated', 'upset', 'upped', 'crucified', 'stood', 'paved', 'forged', 'tidied', 'high-tailed', 'dismissed', 'dubbed', 'revised', 'pondered', 'slackened', 'hated', 'reentered', 'sin-ned', 'contested', 'understood', 'touched', 'dreamt', 'offended', 'decried', 'wasted', 'fancied', 'misted', 'tunneled', 'watered', 'wanted', 'defended', 'suffused', 'Got', 'frustrated', 'addressed', 'dangled', 'leaped', 'esteemed', 'wobbled', 'sponsored', 'regretted', 'filed', 'dripped', 'boosted', 'put', 'knotted', 'decayed', 'gave', 'spoke', 'ate', 'rolled', 'weighed', 'revealed', 'appeared', 'teased', 'issued', 'funneled', 'featured', 'tasted', 'solved', 'measured', 'sailed', 'slammed', 'caught', 'flung', 'bathed', 'stained', 'lured', 'attended', 'grappled', 'resigned', 'dazed', 'transferred', 'despatched', 'echoed', 'shocked', 'enacted', 'elapsed', 'harangued', 'directed', 'dared', 'based', 'controlled', 'censured', 'meted', 'straggled', 'preached', 'suspected', 'realized', 'staked', 'hiked', 'earned', 'fraternized', 'burst', 'vowed', 'retorted', 'copied', 'rivalled', 'voiced', 'summoned', 'valued', 'burned', 'birdied', 'persisted', 'dressed', 'analyzed', 'cradled', 'enclosed', 'christened']]\n"
          ]
        }
      ],
      "source": [
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujpyyq8KGjN9",
        "outputId": "ddeb6a11-b77a-4a23-8466-480fd0efe1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 99.2%\n",
            "\n",
            "Accuracy and first 10 errors per tag:\n",
            " NP 981/1,000 (98.1%) Whiting, mid-April, 400-401, Wilfred, niger, Diffring, aerogenes, Fred, anhemolyticus, Ring\n",
            "NP$ 997/1,000 (99.7%) Nischwitz', Chavez', Grevyles\n",
            "VBG 994/1,000 (99.4%) Followin', Rammin', Shootin', Shippin', Countin', waitin\n",
            "VBD 994/1,000 (99.4%) Sat, Came, Became, Ran, Thought, Got\n"
          ]
        }
      ],
      "source": [
        "# Modify the features generated by this function and run the code cell to see\n",
        "# how your changes affect the accuracy of the classifier.\n",
        "def extract_features(word):\n",
        "  return [\n",
        "      word.endswith('ed'),\n",
        "      word.endswith('ing'),\n",
        "      word.endswith(\"in'\"),\n",
        "      word.endswith(\"s\"),\n",
        "      word.endswith(\"s'\"),\n",
        "      word[0].isupper(),\n",
        "      \"'s\" in word,\n",
        "  ]\n",
        "\n",
        "# The errors listed by this function are words belonging to that tag that were\n",
        "# incorrectly assigned with another tag. Use them to figure out useful features.\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiH2WoVmCyAV"
      },
      "source": [
        "## Question 3\n",
        "Word embeddings can capture semantic and syntactic relationships between words. For example, the vector between the words *king* and *man* is identical to the vector between *queen* and *woman* (i.e., *king* is to *man* as *queen* is to *woman*). This means that if we have a good vector representation for each of those words, we should be able to apply vector arithmetic to find that *king* - *man* + *woman* = *queen*.\n",
        "\n",
        "The function `find_word(a, b, x)`, defined below, finds the word **y**, such that **a** is to **b** as **x** is to **y** (also expressed as **a**:**b** as **x**:**y**).\n",
        "\n",
        "Below, we download GloVe word vectors through Gensim's API. Use those vectors and `find_words()` to complete the following tasks:\n",
        "1. In the UK, people say *petrol* instead of *gas*. Find the British English equivalent of the word *truck*.\n",
        "2. Find the capital of France.\n",
        "3. Find the present tense of the verb *flew*.\n",
        "\n",
        "**Note**: all words in `glove-wiki-gigaword-100` are in lowercase!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRFy2beIqXUa",
        "outputId": "795bd20b-6d89-4042-893f-1048298f0029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def find_word(a, b, x):\n",
        "  # a is to b as x is to ?\n",
        "  a = a.lower()\n",
        "  b = b.lower()\n",
        "  x = x.lower()\n",
        "  print(f\"> {a}:{b} as {x}:?\")\n",
        "  top_words = glove.most_similar_cosmul(positive=[x, b], negative=[a])\n",
        "  for num, (word, score) in enumerate(top_words[:5]):\n",
        "    print(f\"{num + 1}: ({score:.3f}) {word}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iqF6z-8okux",
        "outputId": "0ad71452-bbc6-47c4-f0ca-dfa28de3f886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> man:king as woman:?\n",
            "1: (0.896) queen\n",
            "2: (0.850) monarch\n",
            "3: (0.845) throne\n",
            "4: (0.837) princess\n",
            "5: (0.836) elizabeth\n",
            "\n",
            "> evening:dinner as noon:?\n",
            "1: (0.839) lunch\n",
            "2: (0.829) breakfast\n",
            "3: (0.814) a.m.\n",
            "4: (0.814) p.m.\n",
            "5: (0.813) meal\n",
            "\n",
            "> gas:petrol as truck:?\n",
            "1: (1.020) lorry\n",
            "2: (0.957) wagon\n",
            "3: (0.951) trucks\n",
            "4: (0.950) lorries\n",
            "5: (0.945) car\n",
            "\n",
            "> italy:rome as france:?\n",
            "1: (0.995) paris\n",
            "2: (0.882) prohertrib\n",
            "3: (0.867) strasbourg\n",
            "4: (0.861) brussels\n",
            "5: (0.854) london\n",
            "\n",
            "> went:go as flew:?\n",
            "1: (0.909) fly\n",
            "2: (0.865) flies\n",
            "3: (0.847) planes\n",
            "4: (0.843) flying\n",
            "5: (0.841) arrive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1: man is to king as woman is to ?\n",
        "find_word('man', 'king', 'woman')\n",
        "\n",
        "# Example 2: evening is to dinner as noon is to ?\n",
        "find_word('evening', 'dinner', 'noon')\n",
        "\n",
        "# 1) In the UK, people say 'petrol' instead of 'gas'. Find the British English\n",
        "# equivalent of 'truck'.\n",
        "find_word(\"gas\", \"petrol\", \"truck\")\n",
        "\n",
        "# 2) Find the capital of France. Remember to use only lowercase characters.\n",
        "find_word(\"italy\", \"rome\", \"france\")\n",
        "\n",
        "# 3) Find the present tense of the verb \"flew\".\n",
        "find_word(\"went\", \"go\", \"flew\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-phpyFFCyjL"
      },
      "source": [
        "## Question 4\n",
        "Gensim offers us several ways to find words that are similar or dissimilar to one another. Complete the following tasks:\n",
        "1. Use `glove.most_similar(word, topn=5)` to find the five words that are most similar to:\n",
        "  1. cat\n",
        "  2. samsung\n",
        "  3. batman\n",
        "2. Use `glove.doesnt_match(list_of_strings)` to find which of the words below doesn't fit with the rest:\n",
        "  1. cat hamster gremlin rabbit goldfish dog\n",
        "  2. samsung microsoft dell panasonic mcdonalds facebook\n",
        "  3. batman spiderman daredevil shrek hulk deadpool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYGVE8bHCzA6",
        "outputId": "18b91fd6-45a1-4192-faf6-ea90c4ddc1f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('dog', 0.8798074722290039), ('rabbit', 0.7424427270889282), ('cats', 0.732300341129303), ('monkey', 0.7288709878921509), ('pet', 0.719014048576355)]\n",
            "[('lg', 0.8194022178649902), ('toshiba', 0.7769339084625244), ('hyundai', 0.7322311401367188), ('fujitsu', 0.7246403694152832), ('panasonic', 0.7154008746147156)]\n",
            "[('superman', 0.8058773279190063), ('superhero', 0.6820072531700134), ('sequel', 0.6592288017272949), ('catwoman', 0.654157817363739), ('joker', 0.6362104415893555)]\n",
            "\n",
            "gremlin\n",
            "mcdonalds\n",
            "shrek\n"
          ]
        }
      ],
      "source": [
        "# Your solution here\n",
        "print(glove.most_similar(\"cat\", topn=5))\n",
        "print(glove.most_similar(\"samsung\", topn=5))\n",
        "print(glove.most_similar(\"batman\", topn=5))\n",
        "print()\n",
        "print(glove.doesnt_match(\"cat hamster gremlin rabbit goldfish dog\".split(\" \")))\n",
        "print(glove.doesnt_match(\"samsung microsoft dell panasonic mcdonalds facebook\".split(\" \")))\n",
        "print(glove.doesnt_match(\"batman spiderman daredevil shrek hulk deadpool\".split(\" \")))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
