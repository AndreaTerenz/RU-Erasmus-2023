{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# T-725 Natural Language Processing: Lab 7\n",
        "In today's lab, we will be working with spaCy and Huggingface for a variety of tasks. We'll also learn how to use Gradio to quickly create convenient user interfaces.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* **Select `\"Runtime\" > \"Change runtime type\"`, and make sure that you have \"Hardware accelerator\" set to \"GPU\"**\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ],
      "metadata": {
        "id": "GVUWLZAzmLvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy\n",
        "\n",
        "[spaCy](https://spacy.io) is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning."
      ],
      "metadata": {
        "id": "vvRd4TzTm4uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features\n",
        "\n",
        "Name | Description\n",
        "---|---\n",
        "**Tokenization** | Segmenting text into words, punctuations marks etc.\n",
        "**Part-of-speech (POS) Tagging** | Assigning word types to tokens, like verb or noun.\n",
        "**Dependency Parsing** | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "**Lemmatization** |\tAssigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "**Sentence Boundary Detection (SBD)** |\tFinding and segmenting individual sentences.\n",
        "**Named Entity Recognition (NER)** | Labelling named “real-world” objects, like persons, companies or locations.\n",
        "**Entity Linking (EL)** | Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "**Similarity** | Comparing words, text spans and documents and how similar they are to each other.\n",
        "**Text Classification** | Assigning categories or labels to a whole document, or parts of a document.\n",
        "**Rule-based Matching** | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "**Training** | Updating and improving a statistical model’s predictions.\n",
        "**Serialization** | Saving objects to files or byte strings."
      ],
      "metadata": {
        "id": "i_HhgHSAR5gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trained Pipelines\n",
        "\n",
        "While some of spaCy’s features work independently, others require [trained pipelines](https://spacy.io/models) to be loaded, which enable spaCy to predict linguistic annotations – for example, whether a word is a verb or a noun. A trained pipeline can consist of multiple components that use a statistical model trained on labeled data. spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules."
      ],
      "metadata": {
        "id": "ryBLzQt2R0AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization Example\n",
        "Let's take a look at some of the functionality of spaCy through the example of [automatic summarization](https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25). There are two main types of summarization: extractive and abstractive. Extractive summarization selects a subset of sentences from the text to form a summary; abstractive summarization reorganizes the language in the text and adds novel words/phrases into the summary if necessary.\n",
        "\n",
        "For this example we'll be doing automatic [extractive summarization](https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744)."
      ],
      "metadata": {
        "id": "yGGTyegyYyI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install spaCy:"
      ],
      "metadata": {
        "id": "fQ8XS7evav7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy -q"
      ],
      "metadata": {
        "id": "C18jWsZaTwid"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then import all necessary modules:"
      ],
      "metadata": {
        "id": "owNTDP4lazIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "kCxIVdDk0_cQ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many [other languages](https://spacy.io/usage/models) to choose from. Here we load the English language models:"
      ],
      "metadata": {
        "id": "Y_CC1uaSbAk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "maUkAIjd1ASa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af405ef9-22af-4af4-dde3-9bc805084827"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose some text to be summarized and store it in a variable:"
      ],
      "metadata": {
        "id": "b6Se2Tj0a6z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "Machine learning (ML) is the scientific study of algorithms and statistical models\n",
        "that computer systems use to progressively improve their performance on a specific\n",
        "task. Machine learning algorithms build a mathematical model of sample data, known as\n",
        "“training data”, in order to make predictions or decisions without being explicitly\n",
        "programmed to perform the task. Machine learning algorithms are used in the applications\n",
        "of email filtering, detection of network intruders, and computer vision, where it\n",
        "is infeasible to develop an algorithm of specific instructions for performing the task.\n",
        "Machine learning is closely related to computational statistics, which focuses on\n",
        "making predictions using computers. The study of mathematical optimization delivers\n",
        "methods, theory and application domains to the field of machine learning. Data mining\n",
        "is a field of study within machine learning and focuses on exploratory data analysis\n",
        "through unsupervised learning. In its application across business problems, machine\n",
        "learning is also referred to as predictive analytics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1UejJJP_1NM1"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the text to the `nlp` function:"
      ],
      "metadata": {
        "id": "psGxQfn6bS2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(long_text)"
      ],
      "metadata": {
        "id": "tZ-X-fVX1Qlj"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the text has been processed, i.e., tokenized, lemmatized, tagged with parts-of-speech, and parsed. A variety of lingustic features are accessbile via the `doc` object, e.g.:\n",
        "\n",
        "* Lemmas\n",
        "* Parts of speech\n",
        "* Dependency parse\n",
        "* Named entities\n",
        "* Chunks\n",
        "* Is alphabet character\n",
        "* Is capitalized\n",
        "* Is in the stop list\n",
        "\n",
        "The following will print out each of those bits of information for every token in the original text, one token per line:"
      ],
      "metadata": {
        "id": "s7oEufIlb31T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "CBCarRfubsOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e711e1e7-be06-45c3-9067-20fe4d49d606"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "Machine Machine PROPN NNP compound Xxxxx True False\n",
            "learning learning NOUN NN nsubj xxxx True False\n",
            "( ( PUNCT -LRB- punct ( False False\n",
            "ML ML PROPN NNP appos XX True False\n",
            ") ) PUNCT -RRB- punct ) False False\n",
            "is be AUX VBZ ROOT xx True True\n",
            "the the DET DT det xxx True True\n",
            "scientific scientific ADJ JJ amod xxxx True False\n",
            "study study NOUN NN attr xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "algorithms algorithm NOUN NNS pobj xxxx True False\n",
            "and and CCONJ CC cc xxx True True\n",
            "statistical statistical ADJ JJ amod xxxx True False\n",
            "models model NOUN NNS conj xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "that that SCONJ IN mark xxxx True True\n",
            "computer computer NOUN NN compound xxxx True False\n",
            "systems system NOUN NNS nsubj xxxx True False\n",
            "use use VERB VBP relcl xxx True False\n",
            "to to PART TO aux xx True True\n",
            "progressively progressively ADV RB advmod xxxx True False\n",
            "improve improve VERB VB xcomp xxxx True False\n",
            "their their PRON PRP$ poss xxxx True True\n",
            "performance performance NOUN NN dobj xxxx True False\n",
            "on on ADP IN prep xx True True\n",
            "a a DET DT det x True True\n",
            "specific specific ADJ JJ amod xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "task task NOUN NN pobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Machine machine NOUN NN compound Xxxxx True False\n",
            "learning learn VERB VBG compound xxxx True False\n",
            "algorithms algorithm NOUN NNS nsubj xxxx True False\n",
            "build build VERB VBP ROOT xxxx True False\n",
            "a a DET DT det x True True\n",
            "mathematical mathematical ADJ JJ amod xxxx True False\n",
            "model model NOUN NN dobj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "sample sample NOUN NN compound xxxx True False\n",
            "data datum NOUN NNS pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "known know VERB VBN acl xxxx True False\n",
            "as as ADP IN prep xx True True\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "“ \" PUNCT `` punct “ False False\n",
            "training training NOUN NN compound xxxx True False\n",
            "data datum NOUN NNS pobj xxxx True False\n",
            "” \" PUNCT '' punct ” False False\n",
            ", , PUNCT , punct , False False\n",
            "in in ADP IN prep xx True True\n",
            "order order NOUN NN pobj xxxx True False\n",
            "to to PART TO aux xx True True\n",
            "make make VERB VB acl xxxx True True\n",
            "predictions prediction NOUN NNS dobj xxxx True False\n",
            "or or CCONJ CC cc xx True True\n",
            "decisions decision NOUN NNS conj xxxx True False\n",
            "without without ADP IN prep xxxx True True\n",
            "being be AUX VBG auxpass xxxx True True\n",
            "explicitly explicitly ADV RB advmod xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "programmed program VERB VBN pcomp xxxx True False\n",
            "to to PART TO aux xx True True\n",
            "perform perform VERB VB xcomp xxxx True False\n",
            "the the DET DT det xxx True True\n",
            "task task NOUN NN dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Machine machine NOUN NN compound Xxxxx True False\n",
            "learning learning NOUN NN compound xxxx True False\n",
            "algorithms algorithm NOUN NNS nsubjpass xxxx True False\n",
            "are be AUX VBP auxpass xxx True True\n",
            "used use VERB VBN ROOT xxxx True True\n",
            "in in ADP IN prep xx True True\n",
            "the the DET DT det xxx True True\n",
            "applications application NOUN NNS pobj xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "of of ADP IN prep xx True True\n",
            "email email NOUN NN compound xxxx True False\n",
            "filtering filtering NOUN NN pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "detection detection NOUN NN conj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "network network NOUN NN compound xxxx True False\n",
            "intruders intruder NOUN NNS pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "and and CCONJ CC cc xxx True True\n",
            "computer computer NOUN NN compound xxxx True False\n",
            "vision vision NOUN NN conj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "where where SCONJ WRB advmod xxxx True True\n",
            "it it PRON PRP nsubj xx True True\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "is be AUX VBZ relcl xx True True\n",
            "infeasible infeasible ADJ JJ acomp xxxx True False\n",
            "to to PART TO aux xx True True\n",
            "develop develop VERB VB xcomp xxxx True False\n",
            "an an DET DT det xx True True\n",
            "algorithm algorithm PROPN NNP dobj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "specific specific ADJ JJ amod xxxx True False\n",
            "instructions instruction NOUN NNS pobj xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "performing perform VERB VBG pcomp xxxx True False\n",
            "the the DET DT det xxx True True\n",
            "task task NOUN NN dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "Machine Machine PROPN NNP compound Xxxxx True False\n",
            "learning learning NOUN NN nsubjpass xxxx True False\n",
            "is be AUX VBZ auxpass xx True True\n",
            "closely closely ADV RB advmod xxxx True False\n",
            "related relate VERB VBN ROOT xxxx True False\n",
            "to to ADP IN prep xx True True\n",
            "computational computational ADJ JJ amod xxxx True False\n",
            "statistics statistic NOUN NNS pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "which which PRON WDT nsubj xxxx True True\n",
            "focuses focus VERB VBZ relcl xxxx True False\n",
            "on on ADP IN prep xx True True\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "making make VERB VBG pcomp xxxx True False\n",
            "predictions prediction NOUN NNS dobj xxxx True False\n",
            "using use VERB VBG acl xxxx True True\n",
            "computers computer NOUN NNS dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "The the DET DT det Xxx True True\n",
            "study study NOUN NN nsubj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "mathematical mathematical ADJ JJ amod xxxx True False\n",
            "optimization optimization NOUN NN pobj xxxx True False\n",
            "delivers deliver VERB VBZ ROOT xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "methods method NOUN NNS dobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "theory theory NOUN NN conj xxxx True False\n",
            "and and CCONJ CC cc xxx True True\n",
            "application application NOUN NN compound xxxx True False\n",
            "domains domain NOUN NNS conj xxxx True False\n",
            "to to ADP IN prep xx True True\n",
            "the the DET DT det xxx True True\n",
            "field field NOUN NN pobj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "machine machine NOUN NN compound xxxx True False\n",
            "learning learning NOUN NN pobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Data datum NOUN NNS compound Xxxx True False\n",
            "mining mining NOUN NN nsubj xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "is be AUX VBZ ROOT xx True True\n",
            "a a DET DT det x True True\n",
            "field field NOUN NN attr xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "study study NOUN NN pobj xxxx True False\n",
            "within within ADP IN prep xxxx True True\n",
            "machine machine NOUN NN compound xxxx True False\n",
            "learning learning NOUN NN pobj xxxx True False\n",
            "and and CCONJ CC cc xxx True True\n",
            "focuses focus VERB VBZ conj xxxx True False\n",
            "on on ADP IN prep xx True True\n",
            "exploratory exploratory ADJ JJ amod xxxx True False\n",
            "data datum NOUN NNS compound xxxx True False\n",
            "analysis analysis NOUN NN pobj xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "through through ADP IN prep xxxx True True\n",
            "unsupervised unsupervised ADJ JJ amod xxxx True False\n",
            "learning learning NOUN NN pobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "In in ADP IN prep Xx True True\n",
            "its its PRON PRP$ poss xxx True True\n",
            "application application NOUN NN pobj xxxx True False\n",
            "across across ADP IN prep xxxx True True\n",
            "business business NOUN NN compound xxxx True False\n",
            "problems problem NOUN NNS pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "machine machine NOUN NN compound xxxx True False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n",
            "learning learning NOUN NN nsubjpass xxxx True False\n",
            "is be AUX VBZ auxpass xx True True\n",
            "also also ADV RB advmod xxxx True True\n",
            "referred refer VERB VBN ROOT xxxx True False\n",
            "to to ADP IN prep xx True True\n",
            "as as ADP IN prep xx True True\n",
            "predictive predictive ADJ JJ amod xxxx True False\n",
            "analytics analytic NOUN NNS pobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "\n",
            " \n",
            " SPACE _SP dep \n",
            " False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll use this information to filter keywords from the original text.\n",
        "\n",
        "* Define the keywords list\n",
        "* Choose the parts-of-speech that are likely to be important ([pos tags in spaCy](https://spacy.io/usage/linguistic-features/#pos-tagging))\n",
        "* Skip tokens that are in the stop list\n",
        "* Add tokens that have the part-of-speech we care about to the keywords list"
      ],
      "metadata": {
        "id": "Q3NBen5aeQLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = []\n",
        "pos_tags = [\"PROPN\", \"ADJ\", \"NOUN\", \"VERB\"]\n",
        "for token in doc:\n",
        "    if token.is_stop:\n",
        "        continue\n",
        "    if token.pos_ in pos_tags:\n",
        "        keywords.append(token.text)"
      ],
      "metadata": {
        "id": "wHhJ4dUzebKQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we calculate the frequency of each token using the `Counter` function and store it in `freq_words`.\n",
        "\n",
        "To view the top `n` most frequent words, the `most_common(n)` method can be used:"
      ],
      "metadata": {
        "id": "flCldV1PfwGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_words = Counter(keywords)\n",
        "freq_words.most_common(5)"
      ],
      "metadata": {
        "id": "uudzeyqtfNmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbebbeb4-3da0-49a8-825f-e2e0254ab592"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 8), ('Machine', 4), ('study', 3), ('algorithms', 3), ('task', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This frequency should be normalised for better processing and it can be done by dividing the token's frequencies by the maximum frequency:"
      ],
      "metadata": {
        "id": "PeF7er2qgUJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq = freq_words.most_common(1)[0][1] # most_common(1) returns a 1 element list, containing a tuple where the frequency is the 2nd element\n",
        "for word in freq_words.keys():\n",
        "    freq_words[word] = (freq_words[word]/max_freq)\n",
        "\n",
        "freq_words.most_common(5)"
      ],
      "metadata": {
        "id": "PrRJ5l2Aga-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0479dc-1032-4680-c0af-3211cd7f1a6d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 1.0),\n",
              " ('Machine', 0.5),\n",
              " ('study', 0.375),\n",
              " ('algorithms', 0.375),\n",
              " ('task', 0.375)]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we weigh each sentence based on the frequency of the keyword token present in each sentence. The result is stored as a key-value pair in `sent_strength` where keys are the sentences and the values are the weight of each sentence:"
      ],
      "metadata": {
        "id": "xQLm6730hYoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_strength = {}\n",
        "for sent in doc.sents:\n",
        "    for word in sent:\n",
        "        if word.text in freq_words.keys():\n",
        "            if sent in sent_strength.keys():\n",
        "                sent_strength[sent] += freq_words[word.text]\n",
        "            else:\n",
        "                sent_strength[sent] = freq_words[word.text]\n",
        "\n",
        "print(sent_strength)"
      ],
      "metadata": {
        "id": "MYAwZi2yhwE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd4cce1-9243-4094-8a31-7d8eaa1985e5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "Machine learning (ML) is the scientific study of algorithms and statistical models\n",
            "that computer systems use to progressively improve their performance on a specific\n",
            "task.: 4.125, Machine learning algorithms build a mathematical model of sample data, known as\n",
            "“training data”, in order to make predictions or decisions without being explicitly\n",
            "programmed to perform the task.: 4.625, Machine learning algorithms are used in the applications\n",
            "of email filtering, detection of network intruders, and computer vision, where it\n",
            "is infeasible to develop an algorithm of specific instructions for performing the task.\n",
            ": 4.25, Machine learning is closely related to computational statistics, which focuses on\n",
            "making predictions using computers.: 2.625, The study of mathematical optimization delivers\n",
            "methods, theory and application domains to the field of machine learning.: 3.125, Data mining\n",
            "is a field of study within machine learning and focuses on exploratory data analysis\n",
            "through unsupervised learning.: 4.25, In its application across business problems, machine\n",
            "learning is also referred to as predictive analytics.\n",
            ": 2.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the `nlargest` function is used to summarize the string. It takes 3 arguments:\n",
        "\n",
        "* Number of elements to extract\n",
        "* An Iterable (List/Tuple/Dictionary)\n",
        "* Condition to be satisfied\n",
        "\n",
        "This nlargest function returns a list containing the 3 sentences with the highest sentence strength score calculated in the previous step.\n",
        "\n",
        "We store this output in `summarized_sentences`:"
      ],
      "metadata": {
        "id": "cbmO3Ac-i3zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_sentences = nlargest(4, sent_strength, key=sent_strength.get)\n",
        "\n",
        "print(summarized_sentences)"
      ],
      "metadata": {
        "id": "YJ8N3d0oi3U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172de236-358b-4692-9781-6a88c728f883"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Machine learning algorithms build a mathematical model of sample data, known as\n",
            "“training data”, in order to make predictions or decisions without being explicitly\n",
            "programmed to perform the task., Machine learning algorithms are used in the applications\n",
            "of email filtering, detection of network intruders, and computer vision, where it\n",
            "is infeasible to develop an algorithm of specific instructions for performing the task.\n",
            ", Data mining\n",
            "is a field of study within machine learning and focuses on exploratory data analysis\n",
            "through unsupervised learning., \n",
            "Machine learning (ML) is the scientific study of algorithms and statistical models\n",
            "that computer systems use to progressively improve their performance on a specific\n",
            "task.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, convert the text data in the `summarized_sentences` to a string and print it:"
      ],
      "metadata": {
        "id": "bw3aCNVakFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_sentences = [w.text for w in summarized_sentences]\n",
        "summary = ' '.join(final_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "bCJEnu8IkEYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4c41a5-cc06-4703-fad4-527e0b7aeee1"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning algorithms build a mathematical model of sample data, known as\n",
            "“training data”, in order to make predictions or decisions without being explicitly\n",
            "programmed to perform the task. Machine learning algorithms are used in the applications\n",
            "of email filtering, detection of network intruders, and computer vision, where it\n",
            "is infeasible to develop an algorithm of specific instructions for performing the task.\n",
            " Data mining\n",
            "is a field of study within machine learning and focuses on exploratory data analysis\n",
            "through unsupervised learning. \n",
            "Machine learning (ML) is the scientific study of algorithms and statistical models\n",
            "that computer systems use to progressively improve their performance on a specific\n",
            "task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example only shows a very limited application of [spaCy](https://spacy.io). The package has many powerful tools to create NLP applications."
      ],
      "metadata": {
        "id": "Qiro7L8QB_MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio\n",
        "[Gradio](https://gradio.app) is a fast way to demo your machine learning model with a nice web interface so that anyone can use it. The possibilities with Gradio are vast, this lab only scratches the surface.\n",
        "\n",
        "Here's the setup for a very basic UI:\n",
        "\n",
        "* First, define a function that does your main processing when users click the 'Submit' button in the UI.\n",
        "* Then define a gradio `Interface` called `demo`. This constructor has several arguments:\n",
        "  * The first in this example is the name of the function you defined\n",
        "  * The second is the type of inputs you want to capture (one text input in this case)\n",
        "  * The third is the type of output (also text)\n",
        "* Lastly, call the `Interface` object's `launch()` function to render the UI."
      ],
      "metadata": {
        "id": "Y5H2IZshEfqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "eMMGtUA1pCsV"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "LaQ9oqy4USou"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "9cB8ln93SXvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "559039fd-e09b-4961-d57d-e8de489c2ed6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ed5c81765837343c46.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed5c81765837343c46.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumy"
      ],
      "metadata": {
        "id": "JAZQCBCt5DqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do any kind of processing inside your main function, like call other functions. Let's create a quick summarization tool using [sumy](https://github.com/miso-belica/sumy).\n",
        "\n",
        "* First, install the necessary packages for sumy.\n",
        "* Then import the modules for the summarization task."
      ],
      "metadata": {
        "id": "mCn4OS4UTx5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy -q"
      ],
      "metadata": {
        "id": "UvJiGgbiMZUm"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "id": "SSVtbu4NM_bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a21303f-74a3-4088-c093-68f157416a3c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "LANGUAGE = \"english\"\n",
        "\n",
        "def sumy_summarize(txt, n_sent=1):\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "    summarizer = Summarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "    parser = PlaintextParser.from_string(txt, Tokenizer(LANGUAGE))\n",
        "    sents = \"\"\n",
        "    for sentence in summarizer(parser.document, n_sent):\n",
        "        sents += str(sentence) + \"\\n\"\n",
        "\n",
        "    return sents"
      ],
      "metadata": {
        "id": "-fugZc-eMGNl"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the output of the `sumy_summarize` function inside your main Gradio function, passing it the user input.\n",
        "\n",
        "Try using the text from the previous summarization example as input and experiment with the `n_sent` parameter."
      ],
      "metadata": {
        "id": "yFjvWx5yXiCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(text):\n",
        "    return sumy_summarize(text)\n",
        "\n",
        "sum_demo = gr.Interface(fn=sum, inputs=\"text\", outputs=\"text\")\n",
        "sum_demo.launch(share=True)"
      ],
      "metadata": {
        "id": "70MRhhS0Twwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "7402fd79-f217-4823-c01f-d3b48ef9e896"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ff33e8f7fc3e12ddf6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff33e8f7fc3e12ddf6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `sumy_summarize` function takes another parameter in addition to the text:\n",
        "\n",
        "* n_sent (int or None, optional) – The number of sentences of the original text to be chosen for the summary.\n",
        "\n",
        "Let's add more input elements to the interface:\n",
        "\n",
        "* A checkbox widget allowing the user to capitalize the output\n",
        "* A text box to set the `n_sent`"
      ],
      "metadata": {
        "id": "qrkJephp0VLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(text, make_caps, number_of_sentences):\n",
        "    summary = sumy_summarize(text, n_sent=number_of_sentences)\n",
        "    return summary.upper() if make_caps else summary\n",
        "\n",
        "# The input objects are used in sequence as arguments to the fn function\n",
        "sum_demo = gr.Interface(\n",
        "    fn=sum,\n",
        "    inputs=[\"text\", \"checkbox\", \"number\"],\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "sum_demo.launch(share=True)"
      ],
      "metadata": {
        "id": "4PR6dAf307bQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "fb260d7a-c3b6-468c-873a-ec88229a5003"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c0a92dc379229a3998.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c0a92dc379229a3998.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Interface` is highly customizable. For example, you can give each element different labels, have placeholder text, change colors, etc. Check the [Gradio documentation](https://gradio.app/docs/) for details."
      ],
      "metadata": {
        "id": "f0MFhGObX37G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face\n",
        "\n",
        "\n",
        "🤗 [Transformers](https://huggingface.co/docs/transformers/index) is a state-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX, and provides APIs and tools to easily download and train state-of-the-art pretrained models.\n",
        "\n",
        "Begin by installing the Hugging Face `transformers` library:"
      ],
      "metadata": {
        "id": "alS1iTCWTMWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "8hXjUCHttVIz"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules for the tasks up ahead:"
      ],
      "metadata": {
        "id": "pRWI1tV_qC1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, Conversation"
      ],
      "metadata": {
        "id": "q-3q00aZA10p"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face pipelines make it simple to use any model for inference on any language, computer vision, speech, and multimodal tasks (see [docs](https://huggingface.co/docs/transformers/pipeline_tutorial)). The `pipeline()` automatically loads a default model and a preprocessing class capable of inference for your task.\n",
        "\n",
        "The following is an example of using a Hugging Face pipeline to do automatic abstractive summarization:\n",
        "\n",
        "* First create a pipeline object, here called `summarizer`\n",
        "* The `pipeline()` constructor takes two arguments:\n",
        "  * The name of the task (see [docs for existing pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)) - In our case, the task is 'summarization'\n",
        "  * The model - We can use [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum). This can be changed to any number of models available for this task on Hugging Face.\n",
        "\n",
        "* Next we define a string variable containing the text we want to summarize. Here it's called `text_to_summarize`.\n",
        "* Then call the `summarizer` pipeline, pass the string, and optionally set the max and min length, and method for generation\n",
        "* Finally, print the output"
      ],
      "metadata": {
        "id": "fF918IkJu-Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-xsum\")"
      ],
      "metadata": {
        "id": "VWCSlBv73xdV"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_summarize = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cKSankLlaTsO"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(text_to_summarize, max_length=130, min_length=30, do_sample=False)\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "suj-aGanxeV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8028b05-26f6-4559-b1ed-399327a09f44"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A New York woman has pleaded not guilty to falsely claiming to be married 10 times, including to eight men from different countries, in what prosecutors say was an immigration scam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take things up a notch and create a non-goal-oriented chatbot using pipelines and Gradio.\n",
        "\n",
        "We can define our model and tokenizer explicitly to choose the model best suited for the task, instead of relying on the pipeline's default model. A good model for aimless chit-chat is the [Blenderbot](https://huggingface.co/facebook/blenderbot-400M-distill) model. Note that there are many other chat models you could choose for this task.\n",
        "\n",
        "Start by defining the tokenizer and model:"
      ],
      "metadata": {
        "id": "A-HWgZ51qHkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
        "chat_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
      ],
      "metadata": {
        "id": "UvXhr12zp29P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3956461e-d829-4988-9a04-69b8cffffab4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a 'conversational' pipeline called `bot` using the model and tokenizer we defined above:"
      ],
      "metadata": {
        "id": "HjngaV67jOF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = pipeline(task=\"conversational\", model=chat_model, tokenizer=chat_tokenizer)"
      ],
      "metadata": {
        "id": "-ekGGlgXGdK5"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to build a basic chatbot interface using a pretrained model, we take the following steps:\n",
        "\n",
        "* Initialize the `Conversation` object outside the function, in order to keep building the dialog history over time, so the model has the dialog context when it generates responses.\n",
        "* Define a `chat` function that takes text from user input and keeps a history of the dialog across turns. This function:\n",
        "  * Adds the user's current input to the `Conversation` by calling `add_user_input` on the `convo` object, passing the `input`.\n",
        "  * Passes the `convo` object to the `bot` to make the model generate a response.\n",
        "  * The `bot` object outputs the whole conversation by default, so we call `generated_responses` to get a simple list of the generated responses so far, and get the last item in the list (i.e., the most recent response).\n",
        "  * Then we update the `history`, which is a list of tuples: (user-input, bot-response)\n",
        "* Lastly, we define a Gradio `Interface` that:\n",
        "  * Calls the `chat` function\n",
        "  * Takes text as input\n",
        "  * Creates widgets from a predefined Gradio layout for a 'chatbot'\n",
        "  * Retains a 'state' of the dialog by retaining a list of the chat `history`.\n"
      ],
      "metadata": {
        "id": "JccTsNDJj55M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convo = Conversation()\n",
        "def chat(input, history=[]):\n",
        "    convo.add_user_input(input)\n",
        "    output = bot([convo]).generated_responses[-1]\n",
        "    history.append(input)\n",
        "    history.append(output)\n",
        "    response = [(history[i], history[i+1]) for i in range(0, len(history)-1, 2)]\n",
        "    return response, history\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=chat,\n",
        "    theme=\"default\",\n",
        "    css=\".footer {display:none !important}\",\n",
        "    inputs=[\"text\", \"state\"],\n",
        "    outputs=[\"chatbot\", \"state\"],\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "pHh36Vzgkpg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "3b534637-732b-418a-ee3c-3b3a6df4e428"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0b4f9ba6751b75896d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0b4f9ba6751b75896d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a variety of powerful tools at your disposal to rapidly create interesting and useful NLP applications!"
      ],
      "metadata": {
        "id": "FIb5zlT6zB7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "Complete the following questions and hand in your solution in Canvas before 8:30 Monday morning, October 16th. Remember to save your file before uploading it."
      ],
      "metadata": {
        "id": "76KqmaBLWCh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "Compare the outputs of the three summarization methods covered in this notebook.\n",
        "\n",
        "Use the same piece of text for each method to answer these questions:\n",
        "\n",
        "1. Which method performs the best, in your opinion?\n",
        "2. What are the pros and cons of each method?\n",
        "3. What kind of summarization is each method doing?"
      ],
      "metadata": {
        "id": "vZA9xn7BFl27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here\n",
        "declaration = \"\"\"\n",
        "The unanimous Declaration of the thirteen united States of America, When in the Course of human events,\n",
        "it becomes necessary for one people to dissolve the political bands which have connected them with another,\n",
        "and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature\n",
        "and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should\n",
        "declare the causes which impel them to the separation.\n",
        "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their\n",
        "Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\n",
        "That to secure these rights, Governments are instituted among Men, deriving their just powers from the\n",
        "consent of the governed. That whenever any Form of Government becomes destructive of these ends, it is\n",
        "the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation\n",
        "on such principles and organizing its powers in such form, as to them shall seem most likely to effect\n",
        "their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be\n",
        "changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more\n",
        "disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which\n",
        "they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object\n",
        "evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off\n",
        "such Government, and to provide new Guards for their future security. Such has been the patient sufferance\n",
        "of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government.\n",
        "The history of the present King of Great Britain is a history of repeated injuries and usurpations, all\n",
        "having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts\n",
        "be submitted to a candid world.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m2GikbshFpsJ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_summary(text, sents_count=4, language='en_core_web_sm', pos_tags = [\"PROPN\", \"ADJ\", \"NOUN\", \"VERB\"]):\n",
        "    nlp = spacy.load(language)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    keywords = []\n",
        "    pos_tags = [\"PROPN\", \"ADJ\", \"NOUN\", \"VERB\"]\n",
        "    for token in doc:\n",
        "        if token.is_stop:\n",
        "            continue\n",
        "        if token.pos_ in pos_tags:\n",
        "            keywords.append(token.text)\n",
        "\n",
        "    freq_words = Counter(keywords)\n",
        "    max_freq = freq_words.most_common(1)[0][1]\n",
        "\n",
        "    for word in freq_words.keys():\n",
        "        freq_words[word] = (freq_words[word]/max_freq)\n",
        "\n",
        "    sent_strength = {}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text in freq_words.keys():\n",
        "                if sent in sent_strength.keys():\n",
        "                    sent_strength[sent] += freq_words[word.text]\n",
        "                else:\n",
        "                    sent_strength[sent] = freq_words[word.text]\n",
        "\n",
        "    summarized_sentences = nlargest(sents_count, sent_strength, key=sent_strength.get)\n",
        "\n",
        "    final_sentences = [w.text for w in summarized_sentences]\n",
        "    summary = ' '.join(final_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "GQpBbzOq3pvm"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy_summary(declaration, sents_count=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQyftL_c4gk4",
        "outputId": "dbc4fd1d-0ad3-4471-85da-d83405f4359e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The unanimous Declaration of the thirteen united States of America, When in the Course of human events,\n",
            "it becomes necessary for one people to dissolve the political bands which have connected them with another,\n",
            "and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature\n",
            "and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should\n",
            "declare the causes which impel them to the separation.\n",
            " That whenever any Form of Government becomes destructive of these ends, it is\n",
            "the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation\n",
            "on such principles and organizing its powers in such form, as to them shall seem most likely to effect\n",
            "their Safety and Happiness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sumy_summary(txt, sents_count=1, language=\"english\"):\n",
        "    stemmer = Stemmer(language)\n",
        "    summarizer = Summarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(language)\n",
        "\n",
        "    parser = PlaintextParser.from_string(txt, Tokenizer(language))\n",
        "    sents = \"\"\n",
        "    for sentence in summarizer(parser.document, sents_count):\n",
        "        sents += str(sentence) + \"\\n\"\n",
        "\n",
        "    return sents"
      ],
      "metadata": {
        "id": "OhnpvFQ_4-P2"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sumy_summary(declaration, sents_count=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFcnYMeh5QA8",
        "outputId": "62654b60-a38f-4d03-ed5a-0bec17c6ceec"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\n",
            "But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_summary(txt, min_len=30, max_len=130, model=\"facebook/bart-large-xsum\"):\n",
        "    summarizer = pipeline(\"summarization\", model=model)\n",
        "    summary = summarizer(txt, min_length=min_len, max_length=max_len, do_sample=False)\n",
        "\n",
        "    return summary[0][\"summary_text\"]"
      ],
      "metadata": {
        "id": "Sbpya2kU5vVZ"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformer_summary(declaration, max_len=400))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4WHeOWG6B-J",
        "outputId": "5777112d-2a38-4932-bbd3-0a7cbe198e9a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the text of the Declaration of Independence of the United States of America, signed by all the states of America at the same time, on July 4, 1776.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'd say the best performance came from the HuggingFace summarizer, which actually produced an original text that broadly describes the input, while the others directly copied sentences in their entirety (although those where indeed key concept of the original text)."
      ],
      "metadata": {
        "id": "1G0KdJr660i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "Create a sentiment classifier using Gradio and Huggingface.\n",
        "\n",
        "* Augment the simple version of the Gradio interface.\n",
        "* Add a Huggingface 'text-classification' pipleline, using the 'cardiffnlp/twitter-roberta-base-sentiment' model.\n",
        "\n",
        "This model outputs a list that contains one dictionary object. In this dictionary, the predicted class is the value of the key 'label'. The model outputs one of three sentiment classes:\n",
        "\n",
        "* `LABEL_0` for negative\n",
        "* `LABEL_1` for neutral\n",
        "* `LABEL_2` for positive\n",
        "\n",
        "Your app should take some text as input and output **one** of these three words:\n",
        "\n",
        "* Positive\n",
        "* Neutral\n",
        "* Negative"
      ],
      "metadata": {
        "id": "WDxnK0GhgUto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alter this codeblock and/or create as many blocks as necessary to accomplish the task for this part\n",
        "classif = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "classif(declaration)[0]"
      ],
      "metadata": {
        "id": "9OtdSpNct2eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e3e95f-157b-49e7-c04f-79744ce73ed8"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'LABEL_1', 'score': 0.7346365451812744}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze(text):\n",
        "    res = list(classif(text)[0].values())\n",
        "\n",
        "    label_idx = res[0][-1]\n",
        "    labels = {\n",
        "        \"0\": \"positive\",\n",
        "        \"1\": \"neutral\",\n",
        "        \"2\": \"negative\"\n",
        "    }\n",
        "\n",
        "    return labels[label_idx], res[1]\n",
        "\n",
        "sentiment_analyzer = gr.Interface(\n",
        "    fn=analyze,\n",
        "    inputs=gr.Textbox(label=\"Input text\"),\n",
        "    outputs=[gr.Textbox(label=\"Sentiment label\"), gr.Textbox(label=\"Score\")]\n",
        ")\n",
        "\n",
        "sentiment_analyzer.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "ewafD-9c8E3C",
        "outputId": "d409aaac-746e-4c06-dc80-fc1c532c1362"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://41030362b15c496e76.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://41030362b15c496e76.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3\n",
        "\n",
        "Create a chatbot that can do these three things:\n",
        "\n",
        "*   Summarize text\n",
        "*   Analyze sentiment\n",
        "*   Mindless chit-chat\n",
        "\n",
        "Augment the chatbot code below to accomplish the following:\n",
        "\n",
        "1. When the user ticks a checkbox that says 'do_summary':\n",
        "  * The user's long text input should be summarized by the bot using the pipeline defined earlier and responds with the summary\n",
        "  * This process may bypass the default chit-chat functionality\n",
        "2. When the user ticks a checkbox that says 'do_sentiment':\n",
        "  * The user's text input may be analyzed by the bot for sentiment using the method created in Part 2 and responds accordingly\n",
        "  * This process may bypass the default chit-chat functionality\n",
        "3. Process any other input using the default chat functionality"
      ],
      "metadata": {
        "id": "WBke_7MfuceR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alter this codeblock and/or create as many blocks as necessary to accomplish the task for this part\n",
        "\n",
        "bot = pipeline(task=\"conversational\", model=chat_model, tokenizer=chat_tokenizer)\n",
        "\n",
        "convo = Conversation()\n",
        "def chat(input, history=[]):\n",
        "    convo.add_user_input(input)\n",
        "    output = bot([convo]).generated_responses[-1]\n",
        "    history.append(input)\n",
        "    history.append(output)\n",
        "    response = [(history[i], history[i+1]) for i in range(0, len(history)-1, 2)]\n",
        "    return response, history\n",
        "\n",
        "def main(input, radio):\n",
        "    if radio == \"Sentiment\":\n",
        "        return analyze(input)[0]\n",
        "    elif radio == \"Summary\":\n",
        "        return transformer_summary(input)\n",
        "\n",
        "    return chat(input)[0]\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    theme=\"default\",\n",
        "    css=\".footer {display:none !important}\",\n",
        "    inputs=[\"text\", gr.Radio(\n",
        "        [\"Chat\", \"Summary\", \"Sentiment\"], label=\"What kind of essay would you like to write?\", value=\"Chat\"\n",
        "    )],\n",
        "    outputs=[\"chatbot\"],\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "FE54O0p6f8YI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "7d55e469-4888-4bb7-9bee-b30eedb00dbb"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://fd13b94b199a13633c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fd13b94b199a13633c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 534, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1563, in process_api\n",
            "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1451, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py\", line 262, in postprocess\n",
            "    assert isinstance(\n",
            "AssertionError: Expected a list of lists or list of tuples. Received: A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7891 <> https://fd13b94b199a13633c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    }
  ]
}